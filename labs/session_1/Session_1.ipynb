{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlr112/WebScicence_FinalProject/blob/main/labs/session_1/Session_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xQU9M1D1b7n"
      },
      "outputs": [],
      "source": [
        "# !git config --global user.email \"vlr112@alumni.ku.dk\"\n",
        "# !git config --global user.name \"vlr112\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEnlbhpW1b-H",
        "outputId": "0269b000-9ad3-457f-81c4-3ac9e28a8644"
      },
      "outputs": [],
      "source": [
        "# !git clone https://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@github.com/vlr112/WebScicence_FinalProject.git\n",
        "# note: remove token info before submitting project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MNX8ZqL1cBR",
        "outputId": "a86434b0-c6a2-4c6d-b372-5ee8bbce017c"
      },
      "outputs": [],
      "source": [
        "# %cd WebScicence_FinalProject/labs/session_1/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-w_20vf30cX4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import gzip\n",
        "import numpy as np\n",
        "\n",
        "from pandas.util import hash_pandas_object\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "# #######\n",
        "# # import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import surprise\n",
        "# import regex as re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wSqrrPw00cX7"
      },
      "outputs": [],
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO9PqdMS0cX9"
      },
      "source": [
        "# Familiarize Yourself with the Dataset\n",
        "In the lab sessions, we will work with the \"All Beauty\" category of the Amazon Review Data, and we will use the 5-core subset. You can download the dataset and find information about it here: https://nijianmo.github.io/amazon/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSfoR_ux0C0B"
      },
      "outputs": [],
      "source": [
        "# !pip install git+https://github.com/ru-corporate/sandbox.git@master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URz4VUbL18A4"
      },
      "outputs": [],
      "source": [
        "# %mv /content/All_Beauty_5.json.gz ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOknAhU4O-fV"
      },
      "outputs": [],
      "source": [
        "# !git status\n",
        "# !git add --all\n",
        "# !git commit -a -m \"Exercise lab session1\"\n",
        "# !git remote -v\n",
        "# !git push origin main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQIouQn00cX_"
      },
      "source": [
        "## Exercise 1\n",
        "Download and import the 5-core dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aYOIAYW0wEI"
      },
      "outputs": [],
      "source": [
        "# !wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/All_Beauty_5.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "5aD8CgLXJgSD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['overall', 'verified', 'reviewTime', 'reviewerID', 'asin', 'style',\n",
              "       'reviewerName', 'reviewText', 'summary', 'unixReviewTime', 'vote',\n",
              "       'image'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 214,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def parse(path):\n",
        "  g = gzip.open(path, 'rb')\n",
        "  for l in g:\n",
        "    yield json.loads(l)\n",
        "\n",
        "def getDF(path):\n",
        "  i = 0\n",
        "  df = {}\n",
        "  for d in parse(path):\n",
        "    df[i] = d\n",
        "    i += 1\n",
        "  return pd.DataFrame.from_dict(df, orient='index')\n",
        "  # return pd.DataFrame.from_dict(df)\n",
        "\n",
        "\n",
        "df = getDF('All_Beauty_5.json.gz')\n",
        "\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z4lSMEgDIpCY",
        "outputId": "395aed40-95f2-4299-de40-b3b14682c440"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Data:\n",
        "\n",
        "  def __init__(self,df):\n",
        "\n",
        "    self.df = df\n",
        "\n",
        "  def remove_duplicates(self):\n",
        "\n",
        "    df_clean = self.df[self.df['overall'].notna()]\n",
        "\n",
        "    df_clean = df_clean.sort_values(by =['reviewerID', 'asin', 'unixReviewTime'])\n",
        "    df_clean = df_clean.drop_duplicates(subset = ['reviewerID', 'asin', ], keep = 'last')\n",
        "    # df_clean.reset_index(level=0, inplace=True)\n",
        "    # self.clean_data(df_clean) # to call later \n",
        "    return df_clean.sort_values(by= ['reviewerID', 'unixReviewTime'])\n",
        "\n",
        "\n",
        "  def clean_data(self):\n",
        "\n",
        "    \"\"\" pre test data frame\"\"\"\n",
        "        \n",
        "    df_clean = self.remove_duplicates()\n",
        "\n",
        "    positive_rating = df_clean[df_clean['overall'] >= 4.0]\n",
        "\n",
        "    # I was geting error typeerror unhashable type 'dict' over and over.\n",
        "    # Solution: make extra column with true index, so it won't be lost in \n",
        "    # the cleaning process\n",
        "\n",
        "    # positive_rating.reset_index(level=0, inplace=True)\n",
        "\n",
        "    #sort_it by unixReviewTime and keep most recent\n",
        "\n",
        "    # sorted = positive_rating.sort_values(by= ['reviewerID', 'unixReviewTime']).drop_duplicates(subset=['reviewerID'],keep= 'last')\n",
        "    sorted = positive_rating.drop_duplicates(subset=['reviewerID'],keep= 'last')\n",
        "\n",
        "  \n",
        "    return sorted\n",
        "\n",
        "\n",
        "  def get_train(self):\n",
        "\n",
        "    df_clean = self.remove_duplicates()\n",
        "\n",
        "    pre_test = self.clean_data()\n",
        "\n",
        "    train = df_clean[~df_clean.index.isin(pre_test.index)]\n",
        "\n",
        "    # self.get_test(train) # to call later \n",
        "    return train\n",
        "\n",
        "\n",
        "  def get_test(self):\n",
        "\n",
        "    # df_clean = self.remove_duplicates().set_index('index')\n",
        "    pre_test = self.clean_data()\n",
        "    train = self.get_train()\n",
        "    test = pre_test[pre_test.reviewerID.isin(train.reviewerID)]   \n",
        "    return test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "bebe = Data(df)\n",
        "\n",
        "pre_test = bebe.clean_data()\n",
        "\n",
        "train = bebe.get_train()\n",
        "\n",
        "test = bebe.get_test()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLgs68Fh0cYA"
      },
      "source": [
        "## Exercise 2\n",
        "Clean the dataset from missing ratings and duplicates (cases where the same user has rated the same item multiple times) if any. How many observations does the cleaned dataset have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSm4Tu01I9V1"
      },
      "outputs": [],
      "source": [
        "bebe.remove_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z89NNJ5c0cYB"
      },
      "source": [
        "## Exercise 3\n",
        "Create a test set by extracting the latest (in time) positively rated item (rating $\\geq 4$) by each user. Remove users that do not appear in the training set. How many observations does the training and test set have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cy6zFzuIJAKZ",
        "outputId": "9462d89e-5f59-4c73-b718-a7041d2807ce"
      },
      "outputs": [],
      "source": [
        "print('length train set is : ', len(bebe.get_train())) \n",
        "print('length test set is : ', len(bebe.get_test())) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67CHfBZf0cYC"
      },
      "source": [
        "## Exercise 4\n",
        "### 4.1\n",
        "Compute the number of ratings per user in the training set. What is the summary statistics of the number of ratings, and how does a histogram look like? <br>\n",
        "Reflect on how a collaborative filtering and a content-based recommender system, respectively, will perform for users with few ratings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "_BwbGejR0cYD",
        "outputId": "6f063169-ec3c-4716-bc31-fcc72ed6bcc3"
      },
      "outputs": [],
      "source": [
        "train = bebe.get_train()\n",
        "\n",
        "def plot_dist(df,ff):\n",
        "\n",
        "  ba = df.groupby(ff).agg({'overall': 'count'}).reset_index().drop(ff, axis = 1)\n",
        "  plt.hist(ba['overall'], weights=np.ones(len(ba['overall'])) / len(ba['overall']), bins = range(11))\n",
        "  plt.xticks(range(10))\n",
        "  plt.title(f'Distribution rarings per {ff}')\n",
        "  plt.show()\n",
        "  return ba.describe()\n",
        "\n",
        "plot_dist(train,'reviewerID')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQb4D63N0cYD"
      },
      "source": [
        "### 4.2\n",
        "Compute the number of ratings per item in the training set. How does a barplot of the number of ratings ordered by decreasing frequency look like? <br>\n",
        "Reflect on how it will affect the prediction process of a recommender system if only a small fraction of the items are rated frequently. <br>\n",
        "<br>\n",
        "Repeat this exercise on the test set and reflect on how the evaluation of a recommender system can be affected by popular items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "wKN8pBW20cYD",
        "outputId": "da7287d6-566e-4806-c2cf-6da9d18be856"
      },
      "outputs": [],
      "source": [
        "def ex4_2(train):\n",
        "    s = pd.Series(train['asin'].value_counts(), name='counts')\n",
        "\n",
        "    tr = (s.to_frame())\n",
        "    # tr = tr.sort_values(by = ['counts'], ascending= False)\n",
        "\n",
        "\n",
        "    #new column for ranking. to make a axis\n",
        "    tr['index'] = tr.rank(ascending=False, method='first')\n",
        "\n",
        "    # tr\n",
        "    tr['counts'].plot(kind = 'bar', xticks=tr['index'] )\n",
        "    plt.xticks(range(70))\n",
        "\n",
        "    plt.show();\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ex4_2(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ex4_2(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wby0z-li0cYE"
      },
      "source": [
        "### 4.3\n",
        "Compute the mean rating per user in the training set. What is the summary statistics of the rating means, and how does a histogram look like? <br>\n",
        "Reflect on how a recommender system can take into account if different users rate on different \"scales\" (e.i. a rating of $3$ may be high for one user while low for another).<br>\n",
        "<br>\n",
        "Repeat this exercise with mean rating per item."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#extra\n",
        "\n",
        "def plot_dist_mean(df,ff, plot = False):\n",
        "\n",
        "  ba = df.groupby(ff).agg({'overall': 'mean'}).reset_index().drop(ff, axis = 1)\n",
        "  if plot == True:\n",
        "    plt.hist(ba['overall'], weights=np.ones(len(ba['overall'])) / len(ba['overall']), bins = range(7))\n",
        "    plt.xticks(range(7))\n",
        "    plt.title(f'Distribution rarings per {ff}')\n",
        "    plt.show()\n",
        "  return ba.describe()\n",
        "\n",
        "plot_dist_mean(train,'asin', True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69Y_TnMX0cYE"
      },
      "outputs": [],
      "source": [
        "user= train.groupby('reviewerID').mean('overall').reset_index()\n",
        "print(user['overall'].describe())\n",
        "plt.hist(user['overall'], weights=np.ones(len(user['overall'])) / len(user['overall']), bins = range(7))\n",
        "plt.xticks(range(7))\n",
        "# plt.title(f'Distribution rarings per {ff}')\n",
        "plt.xlabel('Rating means')\n",
        "plt.ylabel('Percentage of users')\n",
        "plt.show();\n",
        "# user['overall'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# item= train.groupby('asin').mean('overall').reset_index()\n",
        "# item['overall'].describe()\n",
        "\n",
        "item = train.groupby('asin').mean('overall').reset_index()\n",
        "print(item['overall'].describe())\n",
        "plt.hist(item['overall'], weights=np.ones(len(item['overall'])) / len(item['overall']), bins = range(7))\n",
        "plt.xticks(range(7))\n",
        "# plt.title(f'Distribution rarings per {ff}')\n",
        "plt.xlabel('Rating means')\n",
        "plt.ylabel('Percentage of items')\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Collaborative Filtering Recommender System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FLw9Rihg4zxG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse.linalg import svds\n",
        "from surprise import Reader\n",
        "from surprise import Dataset\n",
        "\n",
        "\n",
        "import surprise \n",
        "\n",
        "from pandas.io.parsers.readers import read_csv\n",
        "from surprise.model_selection import cross_validate\n",
        "from surprise.model_selection import GridSearchCV\n",
        "\n",
        "from  surprise import KNNWithMeans\n",
        "from surprise import SVD\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqQomEsiLc1_"
      },
      "source": [
        "## Exercise 1\n",
        "In this exercise, we are going to predict the rating of a single user-item pair using a neighborhoodbased\n",
        "method.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW8j20O1Ifg2"
      },
      "source": [
        "### 1.1\n",
        "- Represent the ratings from the training set in a user-item matrix where the rows represent\n",
        "users and the columns represent items.\n",
        "- Fill unobserved ratings with 0.\n",
        "Compute the cosine similarities between the user with ‘reviewerID’=‘A25C2M3QF9G7OQ’ and\n",
        "all users that have rated the item with ‘asin’=‘B00EYZY6LQ’.\n",
        "What are the similarities and what are the ratings given by these users on item ‘B00EYZY6LQ’?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "tMxe9gI-2l7y"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>asin</th>\n",
              "      <th>B000FOI48G</th>\n",
              "      <th>B000GLRREU</th>\n",
              "      <th>B000NKJIXM</th>\n",
              "      <th>B0010ZBORW</th>\n",
              "      <th>B0013NB7DW</th>\n",
              "      <th>B001E96LUO</th>\n",
              "      <th>B001ET7FZE</th>\n",
              "      <th>B001F51RAG</th>\n",
              "      <th>B001LNODUS</th>\n",
              "      <th>B002GP80EU</th>\n",
              "      <th>...</th>\n",
              "      <th>B00EF1QRMU</th>\n",
              "      <th>B00EYZY6LQ</th>\n",
              "      <th>B00L1I1VMG</th>\n",
              "      <th>B00N2WQ2IW</th>\n",
              "      <th>B00W259T7G</th>\n",
              "      <th>B016V8YWBC</th>\n",
              "      <th>B019809F9Y</th>\n",
              "      <th>B019FWRG3C</th>\n",
              "      <th>B01BNEYGQU</th>\n",
              "      <th>B01E7UKR38</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>reviewerID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>A1F7YU6O5RU432</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A1R1BFJCMWX0Y3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A1UQBFCERIP7VJ</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A22CW0ZHY3NJH8</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A25C2M3QF9G7OQ</th>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A2LW5AL0KQ9P1M</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A2PD27UKAD3Q00</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A2WW57XX2UVLM6</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A2ZY49IDE6TY5I</th>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A39WWMBA0299ZF</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A3M6TSEV71537G</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A3S3R88HA0HZG3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A914TQVHI872U</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AOEUN9718KVRD</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "asin            B000FOI48G  B000GLRREU  B000NKJIXM  B0010ZBORW  B0013NB7DW  \\\n",
              "reviewerID                                                                   \n",
              "A1F7YU6O5RU432         0.0         0.0         4.0         0.0         0.0   \n",
              "A1R1BFJCMWX0Y3         0.0         0.0         0.0         0.0         0.0   \n",
              "A1UQBFCERIP7VJ         0.0         0.0         4.0         2.0         0.0   \n",
              "A22CW0ZHY3NJH8         0.0         0.0         0.0         0.0         0.0   \n",
              "A25C2M3QF9G7OQ         5.0         5.0         2.0         0.0         0.0   \n",
              "A2LW5AL0KQ9P1M         0.0         0.0         0.0         0.0         0.0   \n",
              "A2PD27UKAD3Q00         0.0         0.0         0.0         5.0         0.0   \n",
              "A2WW57XX2UVLM6         0.0         0.0         0.0         0.0         0.0   \n",
              "A2ZY49IDE6TY5I         5.0         5.0         0.0         0.0         0.0   \n",
              "A39WWMBA0299ZF         0.0         0.0         0.0         2.0         0.0   \n",
              "A3M6TSEV71537G         0.0         0.0         0.0         0.0         0.0   \n",
              "A3S3R88HA0HZG3         0.0         0.0         0.0         5.0         0.0   \n",
              "A914TQVHI872U          0.0         0.0         0.0         0.0         5.0   \n",
              "AOEUN9718KVRD          0.0         0.0         3.0         0.0         0.0   \n",
              "\n",
              "asin            B001E96LUO  B001ET7FZE  B001F51RAG  B001LNODUS  B002GP80EU  \\\n",
              "reviewerID                                                                   \n",
              "A1F7YU6O5RU432         4.0         0.0         0.0         0.0         0.0   \n",
              "A1R1BFJCMWX0Y3         0.0         0.0         0.0         0.0         3.0   \n",
              "A1UQBFCERIP7VJ         0.0         5.0         0.0         0.0         0.0   \n",
              "A22CW0ZHY3NJH8         4.0         4.0         4.0         0.0         4.0   \n",
              "A25C2M3QF9G7OQ         0.0         0.0         5.0         0.0         0.0   \n",
              "A2LW5AL0KQ9P1M         0.0         0.0         0.0         0.0         3.0   \n",
              "A2PD27UKAD3Q00         0.0         0.0         0.0         5.0         0.0   \n",
              "A2WW57XX2UVLM6         0.0         0.0         0.0         0.0         0.0   \n",
              "A2ZY49IDE6TY5I         0.0         0.0         0.0         0.0         0.0   \n",
              "A39WWMBA0299ZF         0.0         0.0         0.0         5.0         0.0   \n",
              "A3M6TSEV71537G         0.0         0.0         0.0         0.0         0.0   \n",
              "A3S3R88HA0HZG3         0.0         0.0         0.0         0.0         0.0   \n",
              "A914TQVHI872U          0.0         0.0         0.0         0.0         0.0   \n",
              "AOEUN9718KVRD          0.0         0.0         0.0         2.0         0.0   \n",
              "\n",
              "asin            ...  B00EF1QRMU  B00EYZY6LQ  B00L1I1VMG  B00N2WQ2IW  \\\n",
              "reviewerID      ...                                                   \n",
              "A1F7YU6O5RU432  ...         0.0         5.0         5.0         0.0   \n",
              "A1R1BFJCMWX0Y3  ...         0.0         3.0         0.0         0.0   \n",
              "A1UQBFCERIP7VJ  ...         0.0         5.0         0.0         5.0   \n",
              "A22CW0ZHY3NJH8  ...         0.0         3.0         0.0         0.0   \n",
              "A25C2M3QF9G7OQ  ...         0.0         0.0         0.0         0.0   \n",
              "A2LW5AL0KQ9P1M  ...         2.0         4.0         3.0         0.0   \n",
              "A2PD27UKAD3Q00  ...         0.0         5.0         0.0         0.0   \n",
              "A2WW57XX2UVLM6  ...         3.0         4.0         0.0         0.0   \n",
              "A2ZY49IDE6TY5I  ...         0.0         4.0         0.0         0.0   \n",
              "A39WWMBA0299ZF  ...         0.0         5.0         0.0         0.0   \n",
              "A3M6TSEV71537G  ...         0.0         5.0         4.0         4.0   \n",
              "A3S3R88HA0HZG3  ...         5.0         4.0         4.0         5.0   \n",
              "A914TQVHI872U   ...         0.0         5.0         0.0         0.0   \n",
              "AOEUN9718KVRD   ...         0.0         3.0         0.0         0.0   \n",
              "\n",
              "asin            B00W259T7G  B016V8YWBC  B019809F9Y  B019FWRG3C  B01BNEYGQU  \\\n",
              "reviewerID                                                                   \n",
              "A1F7YU6O5RU432         0.0         0.0         0.0         0.0         0.0   \n",
              "A1R1BFJCMWX0Y3         3.0         0.0         0.0         0.0         0.0   \n",
              "A1UQBFCERIP7VJ         0.0         5.0         0.0         0.0         5.0   \n",
              "A22CW0ZHY3NJH8         0.0         0.0         0.0         0.0         4.0   \n",
              "A25C2M3QF9G7OQ         5.0         0.0         0.0         0.0         0.0   \n",
              "A2LW5AL0KQ9P1M         5.0         0.0         0.0         0.0         0.0   \n",
              "A2PD27UKAD3Q00         0.0         0.0         0.0         0.0         0.0   \n",
              "A2WW57XX2UVLM6         0.0         0.0         0.0         0.0         0.0   \n",
              "A2ZY49IDE6TY5I         5.0         0.0         0.0         0.0         0.0   \n",
              "A39WWMBA0299ZF         0.0         0.0         0.0         0.0         0.0   \n",
              "A3M6TSEV71537G         0.0         0.0         0.0         0.0         0.0   \n",
              "A3S3R88HA0HZG3         0.0         0.0         0.0         0.0         0.0   \n",
              "A914TQVHI872U          5.0         0.0         5.0         0.0         0.0   \n",
              "AOEUN9718KVRD          0.0         0.0         0.0         2.0         0.0   \n",
              "\n",
              "asin            B01E7UKR38  \n",
              "reviewerID                  \n",
              "A1F7YU6O5RU432         4.0  \n",
              "A1R1BFJCMWX0Y3         3.0  \n",
              "A1UQBFCERIP7VJ         0.0  \n",
              "A22CW0ZHY3NJH8         0.0  \n",
              "A25C2M3QF9G7OQ         0.0  \n",
              "A2LW5AL0KQ9P1M         0.0  \n",
              "A2PD27UKAD3Q00         0.0  \n",
              "A2WW57XX2UVLM6         0.0  \n",
              "A2ZY49IDE6TY5I         5.0  \n",
              "A39WWMBA0299ZF         0.0  \n",
              "A3M6TSEV71537G         0.0  \n",
              "A3S3R88HA0HZG3         5.0  \n",
              "A914TQVHI872U          0.0  \n",
              "AOEUN9718KVRD          0.0  \n",
              "\n",
              "[14 rows x 24 columns]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#join other target_user with others\n",
        "the_user = train.loc[train['reviewerID'] =='A25C2M3QF9G7OQ']\n",
        "\n",
        "#retrive all users with corresponding item B00EYZY6LQ\n",
        "new = train.loc[train['asin'] =='B00EYZY6LQ']\n",
        "other_users = train.loc[train['reviewerID'].isin(new['reviewerID'])]\n",
        "\n",
        "joint = pd.concat([the_user, other_users])\n",
        "\n",
        "\n",
        "\n",
        "def user_item_matrix(df):\n",
        "\n",
        "  df = df[['reviewerID', 'asin', 'overall']]\n",
        "\n",
        "  all = df.pivot(*df.columns).fillna(0)\n",
        "\n",
        "  return all\n",
        "\n",
        "coisa = user_item_matrix(joint) \n",
        "# coisa['B00W259T7G']\n",
        "len(coisa.columns)\n",
        "coisa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "aBHtnbaE2l-G",
        "outputId": "c3b64fb2-2878-4755-a8df-2b7cbd529f06"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>cosine_similarity</th>\n",
              "      <th>overall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A1F7YU6O5RU432</td>\n",
              "      <td>0.079243</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A1R1BFJCMWX0Y3</td>\n",
              "      <td>0.245145</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A1UQBFCERIP7VJ</td>\n",
              "      <td>0.058634</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A22CW0ZHY3NJH8</td>\n",
              "      <td>0.207883</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A2LW5AL0KQ9P1M</td>\n",
              "      <td>0.275810</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>A2PD27UKAD3Q00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>A2WW57XX2UVLM6</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>A2ZY49IDE6TY5I</td>\n",
              "      <td>0.682835</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>A39WWMBA0299ZF</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>A3M6TSEV71537G</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>A3S3R88HA0HZG3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>A914TQVHI872U</td>\n",
              "      <td>0.245145</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>AOEUN9718KVRD</td>\n",
              "      <td>0.105670</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        reviewerID  cosine_similarity  overall\n",
              "0   A1F7YU6O5RU432           0.079243      5.0\n",
              "1   A1R1BFJCMWX0Y3           0.245145      3.0\n",
              "2   A1UQBFCERIP7VJ           0.058634      5.0\n",
              "3   A22CW0ZHY3NJH8           0.207883      3.0\n",
              "4   A2LW5AL0KQ9P1M           0.275810      4.0\n",
              "5   A2PD27UKAD3Q00           0.000000      5.0\n",
              "6   A2WW57XX2UVLM6           0.000000      4.0\n",
              "7   A2ZY49IDE6TY5I           0.682835      4.0\n",
              "8   A39WWMBA0299ZF           0.000000      5.0\n",
              "9   A3M6TSEV71537G           0.000000      5.0\n",
              "10  A3S3R88HA0HZG3           0.000000      4.0\n",
              "11   A914TQVHI872U           0.245145      5.0\n",
              "12   AOEUN9718KVRD           0.105670      3.0"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def cosine_sim(df, new):\n",
        "\n",
        "  bab = pd.DataFrame(cosine_similarity(df))\n",
        "  bab.columns = coisa.index\n",
        "  bab.index = coisa.index\n",
        "\n",
        "  bab = pd.DataFrame(bab['A25C2M3QF9G7OQ']).drop('A25C2M3QF9G7OQ').reset_index()\n",
        "\n",
        "  bab.columns = ['reviewerID', 'cosine_similarity']\n",
        "  bab =bab.sort_values(by = ['reviewerID']).reset_index()\n",
        "\n",
        "  new = new.sort_values(by = ['reviewerID']).reset_index()\n",
        "\n",
        "\n",
        "  bab['overall'] = pd.DataFrame(new['overall'])\n",
        "\n",
        "  return bab.drop(['index'], axis = 1)\n",
        "\n",
        "oi =cosine_sim(coisa, new)\n",
        "\n",
        "oi\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "oi.sort_values(by =['cosine_similarity'], ascending=False).head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn5F193pIZ4t"
      },
      "source": [
        "### 1.2\n",
        "Predict the rating for user ‘A25C2M3QF9G7OQ’ on item ‘B00EYZY6LQ’ based on the ratings from\n",
        "the 3 most similar users, using a weighted (by similarity) average. What is the prediction?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "ujkYCNFN2mAQ",
        "outputId": "e6dc1d72-4678-4570-a0b4-57d3a4e20626"
      },
      "outputs": [],
      "source": [
        "# import data from solutions sheet :) \n",
        "#COMPARE WITH MY OWN RESULTS\n",
        "\n",
        "data = read_csv('results.txt', sep = ' ')\n",
        "\n",
        "data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "gj94lQ3ZAbi9",
        "outputId": "4ba66974-991c-4357-d01e-fb486c53acee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.7963554954121093"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def predict(data):\n",
        "\n",
        "    # data.colunames = ['reviewerID', 'cosine_similarity', 'overall']\n",
        "    data = data.sort_values(by = ['cosine_similarity'], ascending= False).head(3)\n",
        "\n",
        "\n",
        "    #FOR THIS PURPOSE I'LL USE RESULTS.TXT\n",
        "\n",
        "    data['new'] = data['cosine_similarity'] * data['overall']\n",
        "\n",
        "    return sum(data['new'])/ sum(data['cosine_similarity'])\n",
        "\n",
        "# predict(data)\n",
        "prep = oi.sort_values(by =['cosine_similarity'], ascending=False).head(3)\n",
        "\n",
        "predict(prep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2\n",
        "In this exercise, we are going to predict the rating of the same user-item pair as in exercise 1, now\n",
        "using a latent factor method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1\n",
        "- Represent the ratings from the training set in a user-item matrix where the rows represent\n",
        "users and the columns represent items.\n",
        "- Subtract the row mean (i.e. mean rating per user) from each non-missing element in the\n",
        "matrix.\n",
        "- Replace missing values with 0.\n",
        "Factorize the user-item matrix by performing Singular Value Decomposition (SVD) of rank 5 using\n",
        "eigendecomposition. What is ther user factors of user ‘A25C2M3QF9G7OQ’ and the item factors\n",
        "of item ‘B00EYZY6LQ’?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def user_item_matrix_2_1(df):\n",
        "\n",
        "  df = df[['reviewerID', 'asin', 'overall']]\n",
        "\n",
        "  all = df.pivot(*df.columns)\n",
        "\n",
        "  #add mean collumn \n",
        "  all['mean'] = all.mean(axis=1)\n",
        "\n",
        "  #Subtract the row mean (i.e. mean rating per user) from each non-missing element in the matrix.\n",
        "  #Replace missing values with 0.\n",
        "  all = all.subtract(all['mean'], axis = 0).fillna(0)\n",
        "\n",
        "\n",
        "  return all.drop(['mean'], axis=1)\n",
        "\n",
        "ratings_matrix_2_1 = user_item_matrix_2_1(joint) \n",
        "\n",
        "ratings_matrix_2_1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from scipy.sparse.linalg import svds\n",
        "import scipy\n",
        "# import numpy.random.RandomState as random\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SVD usiing scipy pkg\n",
        "# import scipy\n",
        "\n",
        "u , s ,vT = scipy.sparse.linalg.svds(ratings_matrix_2_1, solver='arpack', k = 5)\n",
        "\n",
        "\n",
        "A2 = u @ np.diag(s) @ vT\n",
        "\n",
        "pd.DataFrame(A2)\n",
        "\n",
        "np.allclose(A2, ratings_matrix_2_1.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SVD from scratch\n",
        "q, sigma, vT = np.linalg.svd(ratings_matrix_2_1, full_matrices=False)\n",
        "\n",
        "user_factors = np.matmul(q, np.diag(sigma))\n",
        "pd.DataFrame(user_factors)\n",
        "\n",
        "item_factors = pd.DataFrame(vT)\n",
        "item_factors\n",
        "# user_factors = (q[:,:5])\n",
        "# pd.DataFrame(user_factors)\n",
        "\n",
        "\n",
        "\n",
        "# jj = pd.DataFrame(vT[:5])\n",
        "\n",
        "# jj[14]\n",
        "\n",
        "# ori = np.matmul(user_factors, vT[:5])\n",
        "\n",
        "# pd.DataFrame(ori)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#SVD using surprise p\n",
        "\n",
        "svd = SVD(verbose=True, n_epochs=10)\n",
        "trainset = data.build_full_trainset()\n",
        "svd.fit(trainset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svd = TruncatedSVD(n_components=5, n_iter=1)\n",
        "new_ratings = svd.fit_transform(ratings_matrix_2_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.shape(new_ratings)\n",
        "pd.DataFrame(new_ratings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ratings_matrix_2_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_ratings.feature_names_in_\n",
        "svd.components_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOX0_j7tInQT"
      },
      "source": [
        "## Exercise 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFvOOAmQIwD_"
      },
      "source": [
        "### 3.1\n",
        "Define a user-based neighborhood model that takes into account the **mean rating of each user**.\n",
        "Use **cosine as similarity measure** and try to vary the (maximum) number of neighbors to take into\n",
        "account when predicting ratings. Keep Scikit-Surprise’s default setting for all other parameters.\n",
        "Is it better to use 1 or 10 neighbors? You should determine this based on the Root Mean Square\n",
        "Error (RMSE) over 3-fold cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MTjVN9smCflX"
      },
      "outputs": [],
      "source": [
        "reader = Reader(rating_scale=(1, 5))\n",
        "\n",
        "data = Dataset.load_from_df(train[['reviewerID', 'asin', 'overall']], reader)\n",
        "\n",
        "trainset = data.build_full_trainset()\n",
        "\n",
        "# Return a list of ratings that can be used as a testset in the test() method.\n",
        "# The ratings are all the ratings that are not in the trainset, i.e. all the ratings rui where the user u is known, the item i is known, but the rating rui is not in the trainset. \n",
        "anti_test = trainset.build_anti_testset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainset.n_ratings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx1TEeJUC-vZ",
        "outputId": "655650d2-79f5-4565-e233-871723122ec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.46613174711993216\n",
            "{'k': 10, 'sim_options': {'name': 'cosine', 'user_based': True}, 'verbose': False}\n"
          ]
        }
      ],
      "source": [
        "param_grid = {'k': [1, 10],\n",
        "              'sim_options': {'name': ['cosine'],\n",
        "                              # 'min_support': [1, 5],\n",
        "                              'user_based': [True]},\n",
        "                'verbose': [False]\n",
        "              }\n",
        "\n",
        "gs = GridSearchCV(KNNWithMeans, param_grid, measures=['rmse'], cv=3)\n",
        "\n",
        "gs.fit(data)\n",
        "\n",
        "# best RMSE score\n",
        "print(gs.best_score['rmse'])\n",
        "\n",
        "# combination of parameters that gave the best RMSE score\n",
        "print(gs.best_params['rmse'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5H4CFKn5IwGQ"
      },
      "outputs": [],
      "source": [
        "## https://bmanohar16.github.io/blog/recsys-evaluation-in-surprise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL7jcwPVHw9_"
      },
      "source": [
        "### 3.2\n",
        "**Fit the neigborhood-based model** defined in exercise 3.1 on the **full training set** with cosine as\n",
        "similarity measure **bold text** and either 1 or 10 neighbors based on what you found to be better in exercise\n",
        "3.1. Keep Scikit-Surprise’s default setting for all other parameters, but set the random state to 0\n",
        "for comparable results.\n",
        "Use the model to predict the unobserved ratings for the users in the training set. How many\n",
        "predictions are there and what is the average of all the predictions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainset.all_users()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "yOclyexY8qIw",
        "outputId": "9dff1e42-f2de-42f9-f227-76f4b4b8d197"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Number of predictions:  54746\n"
          ]
        }
      ],
      "source": [
        "trainset = data.build_full_trainset()\n",
        "anti_test = trainset.build_anti_testset()\n",
        "testset = trainset.build_testset()\n",
        "\n",
        "\n",
        "# data_test = Dataset.load_from_df(test[['reviewerID', 'asin', 'overall']], reader)\n",
        "# trainset_test = data_test.build_full_trainset()\n",
        "\n",
        "sim_options= {'k': '10',\n",
        "              'name': 'cosine',\n",
        "              # 'min_support': [1, 5],\n",
        "              'user_based': [True]}\n",
        "\n",
        "\n",
        "alg_nbm = KNNWithMeans(sim_options= sim_options, random_state = 0, verbose = True)\n",
        "\n",
        "# nbm -> neigborhood based model\n",
        "predictions_nbm = alg_nbm.fit(trainset).test(anti_test)\n",
        "\n",
        "print('Number of predictions: ',len(predictions_nbm) )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uid</th>\n",
              "      <th>iid</th>\n",
              "      <th>r_ui</th>\n",
              "      <th>est</th>\n",
              "      <th>details</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A105A034ZG9EHO</td>\n",
              "      <td>B00W259T7G</td>\n",
              "      <td>4.722311</td>\n",
              "      <td>3.1</td>\n",
              "      <td>{'actual_k': 2, 'was_impossible': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A105A034ZG9EHO</td>\n",
              "      <td>B000VV1YOY</td>\n",
              "      <td>4.722311</td>\n",
              "      <td>5.0</td>\n",
              "      <td>{'actual_k': 0, 'was_impossible': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A105A034ZG9EHO</td>\n",
              "      <td>B001LNODUS</td>\n",
              "      <td>4.722311</td>\n",
              "      <td>5.0</td>\n",
              "      <td>{'actual_k': 0, 'was_impossible': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A105A034ZG9EHO</td>\n",
              "      <td>B00006L9LC</td>\n",
              "      <td>4.722311</td>\n",
              "      <td>5.0</td>\n",
              "      <td>{'actual_k': 40, 'was_impossible': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A105A034ZG9EHO</td>\n",
              "      <td>B0012Y0ZG2</td>\n",
              "      <td>4.722311</td>\n",
              "      <td>5.0</td>\n",
              "      <td>{'actual_k': 40, 'was_impossible': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54741</th>\n",
              "      <td>AZRD4IZU6TBFV</td>\n",
              "      <td>B001QY8QXM</td>\n",
              "      <td>4.722311</td>\n",
              "      <td>5.0</td>\n",
              "      <td>{'actual_k': 0, 'was_impossible': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54742</th>\n",
              "      <td>AZRD4IZU6TBFV</td>\n",
              "      <td>B00RZYW4RG</td>\n",
              "      <td>4.722311</td>\n",
              "      <td>5.0</td>\n",
              "      <td>{'actual_k': 1, 'was_impossible': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54743</th>\n",
              "      <td>AZRD4IZU6TBFV</td>\n",
              "      <td>B007V6JNE0</td>\n",
              "      <td>4.722311</td>\n",
              "      <td>5.0</td>\n",
              "      <td>{'actual_k': 0, 'was_impossible': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54744</th>\n",
              "      <td>AZRD4IZU6TBFV</td>\n",
              "      <td>B000X2FPXC</td>\n",
              "      <td>4.722311</td>\n",
              "      <td>5.0</td>\n",
              "      <td>{'actual_k': 0, 'was_impossible': False}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54745</th>\n",
              "      <td>AZRD4IZU6TBFV</td>\n",
              "      <td>B00126LYJM</td>\n",
              "      <td>4.722311</td>\n",
              "      <td>5.0</td>\n",
              "      <td>{'actual_k': 1, 'was_impossible': False}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>54746 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  uid         iid      r_ui  est  \\\n",
              "0      A105A034ZG9EHO  B00W259T7G  4.722311  3.1   \n",
              "1      A105A034ZG9EHO  B000VV1YOY  4.722311  5.0   \n",
              "2      A105A034ZG9EHO  B001LNODUS  4.722311  5.0   \n",
              "3      A105A034ZG9EHO  B00006L9LC  4.722311  5.0   \n",
              "4      A105A034ZG9EHO  B0012Y0ZG2  4.722311  5.0   \n",
              "...               ...         ...       ...  ...   \n",
              "54741   AZRD4IZU6TBFV  B001QY8QXM  4.722311  5.0   \n",
              "54742   AZRD4IZU6TBFV  B00RZYW4RG  4.722311  5.0   \n",
              "54743   AZRD4IZU6TBFV  B007V6JNE0  4.722311  5.0   \n",
              "54744   AZRD4IZU6TBFV  B000X2FPXC  4.722311  5.0   \n",
              "54745   AZRD4IZU6TBFV  B00126LYJM  4.722311  5.0   \n",
              "\n",
              "                                         details  \n",
              "0       {'actual_k': 2, 'was_impossible': False}  \n",
              "1       {'actual_k': 0, 'was_impossible': False}  \n",
              "2       {'actual_k': 0, 'was_impossible': False}  \n",
              "3      {'actual_k': 40, 'was_impossible': False}  \n",
              "4      {'actual_k': 40, 'was_impossible': False}  \n",
              "...                                          ...  \n",
              "54741   {'actual_k': 0, 'was_impossible': False}  \n",
              "54742   {'actual_k': 1, 'was_impossible': False}  \n",
              "54743   {'actual_k': 0, 'was_impossible': False}  \n",
              "54744   {'actual_k': 0, 'was_impossible': False}  \n",
              "54745   {'actual_k': 1, 'was_impossible': False}  \n",
              "\n",
              "[54746 rows x 5 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "4.628158995296572"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# for ease make it a df\n",
        "pred_nbm_df = pd.DataFrame(predictions_nbm)\n",
        "pred_nbm_df\n",
        "np.mean(pred_nbm_df['est'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### note: prediction varies slighttly. is it very relevant?? must ask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copied code!\n",
        "\n",
        "def get_Iu(uid):\n",
        "    \"\"\" return the number of items rated by given user\n",
        "    args: \n",
        "      uid: the id of the user\n",
        "    returns: \n",
        "      the number of items rated by the user\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return len(trainset.ur[trainset.to_inner_uid(uid)])\n",
        "    except ValueError: # user was not part of the trainset\n",
        "        return 0\n",
        "    \n",
        "def get_Ui(iid):\n",
        "    \"\"\" return number of users that have rated given item\n",
        "    args:\n",
        "      iid: the raw id of the item\n",
        "    returns:\n",
        "      the number of users that have rated the item.\n",
        "    \"\"\"\n",
        "    try: \n",
        "        return len(trainset.ir[trainset.to_inner_iid(iid)])\n",
        "    except ValueError:\n",
        "        return 0\n",
        "    \n",
        "df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])\n",
        "df['Iu'] = df.uid.apply(get_Iu)\n",
        "df['Ui'] = df.iid.apply(get_Ui)\n",
        "df['err'] = abs(df.est - df.rui)\n",
        "best_predictions = df.sort_values(by='err')[:10]\n",
        "worst_predictions = df.sort_values(by='err')[-10:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBCjftzsP2bJ"
      },
      "source": [
        "## Exercise 4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO30WBk3PxT8"
      },
      "source": [
        "### 4.1\n",
        "Define an SVD model with user and item biases that uses Stochastic Gradient Descend (SGD) to\n",
        "estimate the low-rank matrix based on only observed ratings.\n",
        "Set the number of latent factors to 30 and try to iterate the SGD procedure for different number of\n",
        "epochs. Keep Scikit-Surprise’s default setting for all other parameters.\n",
        "Is it better to run for 100 or 500 epochs? You should determine this based on the RMSE over 3-fold\n",
        "cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZJjMmNpP7-d",
        "outputId": "62f68a9b-1f07-4300-c47a-c23079e41e78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3580472964553141\n",
            "{'n_epochs': 500, 'n_factors': 30, 'verbose': False}\n"
          ]
        }
      ],
      "source": [
        "param_grid = {'n_epochs': [100, 500],\n",
        "              'n_factors' : [30],\n",
        "              'verbose' : [False]}\n",
        "\n",
        "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
        "\n",
        "gs.fit(data)\n",
        "\n",
        "# best RMSE score\n",
        "print(gs.best_score['rmse'])\n",
        "\n",
        "# combination of parameters that gave the best RMSE score\n",
        "print(gs.best_params['rmse'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2\n",
        "Fit the latent factor model defined in exercise 4.1 on the full training set with 30 latent factors and run for either 100 or 500 epochs based on what you found to be better in exercise 4.1. Keep Scikit- Surprise’s default setting for all other parameters, but set the random state to 0 for comparable results.\n",
        "Use the model to predict the unobserved ratings for the users in the training set. How many predictions are there and what is the average of all the predictions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "w2e7zsQRP8Hd"
      },
      "outputs": [],
      "source": [
        "# trainset = data.build_full_trainset()\n",
        "# anti_test = trainset.build_anti_testset()\n",
        "\n",
        "alg_svd = SVD( n_factors = 30, n_epochs = 500,  random_state= 0, verbose = False)\n",
        "\n",
        "#mdm -> model based model\n",
        "predictions_mbm = alg_svd.fit(trainset).test(anti_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4.403720461682837"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_mbm__df = pd.DataFrame(predictions_mbm)\n",
        "np.mean(pred_mbm__df['est'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_mbm__df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation of Recommender Systems\n",
        "\n",
        "Based on the same dataset used on previous weeks, let us evaluate the Collaborative Filtering (CF) models implemented last week."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import surprise\n",
        "# from surprise import evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1\n",
        "\n",
        "1. Load the test set and the predictions made with both Collaborative Filtering models in the previous session. \n",
        "2. Detect those users which are in the training set but not in the test set. Remove their predictions before evaluating the systems.\n",
        "3. Report the Root Mean Square Error (RMSE) for both CF models defined in the previous session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>verified</th>\n",
              "      <th>reviewTime</th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>asin</th>\n",
              "      <th>style</th>\n",
              "      <th>reviewerName</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>summary</th>\n",
              "      <th>unixReviewTime</th>\n",
              "      <th>vote</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3887</th>\n",
              "      <td>5.0</td>\n",
              "      <td>True</td>\n",
              "      <td>07 6, 2014</td>\n",
              "      <td>A105A034ZG9EHO</td>\n",
              "      <td>B0012Y0ZG2</td>\n",
              "      <td>{'Size:': ' 180'}</td>\n",
              "      <td>K. Mras</td>\n",
              "      <td>yum</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>1404604800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4005</th>\n",
              "      <td>5.0</td>\n",
              "      <td>True</td>\n",
              "      <td>08 13, 2013</td>\n",
              "      <td>A10JB7YPWZGRF4</td>\n",
              "      <td>B0012Y0ZG2</td>\n",
              "      <td>{'Size:': ' 45'}</td>\n",
              "      <td>Amazon Customer</td>\n",
              "      <td>I continually get compliments on how wonderful...</td>\n",
              "      <td>Heaven !</td>\n",
              "      <td>1376352000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5030</th>\n",
              "      <td>5.0</td>\n",
              "      <td>False</td>\n",
              "      <td>09 6, 2017</td>\n",
              "      <td>A10M2MLE2R0L6K</td>\n",
              "      <td>B019FWRG3C</td>\n",
              "      <td>{'Color:': ' Bath Salts'}</td>\n",
              "      <td>Booklover</td>\n",
              "      <td>I am a bath person.  I always have been.  I lo...</td>\n",
              "      <td>Wonderful lavender scent</td>\n",
              "      <td>1504656000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3560</th>\n",
              "      <td>5.0</td>\n",
              "      <td>True</td>\n",
              "      <td>03 16, 2016</td>\n",
              "      <td>A10P0NAKKRYKTZ</td>\n",
              "      <td>B0012Y0ZG2</td>\n",
              "      <td>{'Size:': ' 97'}</td>\n",
              "      <td>Amazon Customer</td>\n",
              "      <td>Fantastic shower gel. Not only lathers well bu...</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>1458086400</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4280</th>\n",
              "      <td>5.0</td>\n",
              "      <td>True</td>\n",
              "      <td>04 28, 2017</td>\n",
              "      <td>A10ZJZNO4DAVB</td>\n",
              "      <td>B001OHV1H4</td>\n",
              "      <td>{'Size:': ' 43'}</td>\n",
              "      <td>Loeyd</td>\n",
              "      <td>What the hubby wanted</td>\n",
              "      <td>Love it</td>\n",
              "      <td>1493337600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4605</th>\n",
              "      <td>5.0</td>\n",
              "      <td>True</td>\n",
              "      <td>08 4, 2014</td>\n",
              "      <td>AZCOSCQG73JZ1</td>\n",
              "      <td>B001OHV1H4</td>\n",
              "      <td>{'Size:': ' B-013'}</td>\n",
              "      <td>william</td>\n",
              "      <td>extremely pleased, very pleasant scent, very l...</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>1407110400</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4033</th>\n",
              "      <td>5.0</td>\n",
              "      <td>True</td>\n",
              "      <td>05 26, 2013</td>\n",
              "      <td>AZD3ON9ZMEGL6</td>\n",
              "      <td>B0012Y0ZG2</td>\n",
              "      <td>{'Size:': ' 124'}</td>\n",
              "      <td>huangweixiong</td>\n",
              "      <td>It smells good, suitable for my needs, the pri...</td>\n",
              "      <td>i love it</td>\n",
              "      <td>1369526400</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4471</th>\n",
              "      <td>5.0</td>\n",
              "      <td>True</td>\n",
              "      <td>12 29, 2015</td>\n",
              "      <td>AZFYUPGEE6KLW</td>\n",
              "      <td>B001OHV1H4</td>\n",
              "      <td>{'Size:': ' 483'}</td>\n",
              "      <td>Jo Kamcy</td>\n",
              "      <td>Love this.  I can't find it in the makeup stor...</td>\n",
              "      <td>Love this. I can't find it in the makeup ...</td>\n",
              "      <td>1451347200</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4634</th>\n",
              "      <td>5.0</td>\n",
              "      <td>True</td>\n",
              "      <td>12 20, 2013</td>\n",
              "      <td>AZJMUP77WBQZQ</td>\n",
              "      <td>B001OHV1H4</td>\n",
              "      <td>{'Size:': ' 329'}</td>\n",
              "      <td>S. Foote</td>\n",
              "      <td>THIS WAS A GIFT PURCHASED LAST YEAR FOR MY DAU...</td>\n",
              "      <td>GIFT</td>\n",
              "      <td>1387497600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3848</th>\n",
              "      <td>5.0</td>\n",
              "      <td>True</td>\n",
              "      <td>09 28, 2014</td>\n",
              "      <td>AZRD4IZU6TBFV</td>\n",
              "      <td>B0012Y0ZG2</td>\n",
              "      <td>{'Size:': ' 200'}</td>\n",
              "      <td>Norma Gandy</td>\n",
              "      <td>Like this product very much..it smells great.</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>1411862400</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>949 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      overall  verified   reviewTime      reviewerID        asin  \\\n",
              "3887      5.0      True   07 6, 2014  A105A034ZG9EHO  B0012Y0ZG2   \n",
              "4005      5.0      True  08 13, 2013  A10JB7YPWZGRF4  B0012Y0ZG2   \n",
              "5030      5.0     False   09 6, 2017  A10M2MLE2R0L6K  B019FWRG3C   \n",
              "3560      5.0      True  03 16, 2016  A10P0NAKKRYKTZ  B0012Y0ZG2   \n",
              "4280      5.0      True  04 28, 2017   A10ZJZNO4DAVB  B001OHV1H4   \n",
              "...       ...       ...          ...             ...         ...   \n",
              "4605      5.0      True   08 4, 2014   AZCOSCQG73JZ1  B001OHV1H4   \n",
              "4033      5.0      True  05 26, 2013   AZD3ON9ZMEGL6  B0012Y0ZG2   \n",
              "4471      5.0      True  12 29, 2015   AZFYUPGEE6KLW  B001OHV1H4   \n",
              "4634      5.0      True  12 20, 2013   AZJMUP77WBQZQ  B001OHV1H4   \n",
              "3848      5.0      True  09 28, 2014   AZRD4IZU6TBFV  B0012Y0ZG2   \n",
              "\n",
              "                          style     reviewerName  \\\n",
              "3887          {'Size:': ' 180'}          K. Mras   \n",
              "4005           {'Size:': ' 45'}  Amazon Customer   \n",
              "5030  {'Color:': ' Bath Salts'}        Booklover   \n",
              "3560           {'Size:': ' 97'}  Amazon Customer   \n",
              "4280           {'Size:': ' 43'}            Loeyd   \n",
              "...                         ...              ...   \n",
              "4605        {'Size:': ' B-013'}          william   \n",
              "4033          {'Size:': ' 124'}    huangweixiong   \n",
              "4471          {'Size:': ' 483'}         Jo Kamcy   \n",
              "4634          {'Size:': ' 329'}         S. Foote   \n",
              "3848          {'Size:': ' 200'}      Norma Gandy   \n",
              "\n",
              "                                             reviewText  \\\n",
              "3887                                                yum   \n",
              "4005  I continually get compliments on how wonderful...   \n",
              "5030  I am a bath person.  I always have been.  I lo...   \n",
              "3560  Fantastic shower gel. Not only lathers well bu...   \n",
              "4280                              What the hubby wanted   \n",
              "...                                                 ...   \n",
              "4605  extremely pleased, very pleasant scent, very l...   \n",
              "4033  It smells good, suitable for my needs, the pri...   \n",
              "4471  Love this.  I can't find it in the makeup stor...   \n",
              "4634  THIS WAS A GIFT PURCHASED LAST YEAR FOR MY DAU...   \n",
              "3848      Like this product very much..it smells great.   \n",
              "\n",
              "                                           summary  unixReviewTime vote image  \n",
              "3887                                    Five Stars      1404604800  NaN   NaN  \n",
              "4005                                      Heaven !      1376352000  NaN   NaN  \n",
              "5030                      Wonderful lavender scent      1504656000  NaN   NaN  \n",
              "3560                                    Five Stars      1458086400  NaN   NaN  \n",
              "4280                                       Love it      1493337600  NaN   NaN  \n",
              "...                                            ...             ...  ...   ...  \n",
              "4605                                    Five Stars      1407110400  NaN   NaN  \n",
              "4033                                     i love it      1369526400  NaN   NaN  \n",
              "4471  Love this. I can't find it in the makeup ...      1451347200  NaN   NaN  \n",
              "4634                                          GIFT      1387497600  NaN   NaN  \n",
              "3848                                    Five Stars      1411862400  NaN   NaN  \n",
              "\n",
              "[949 rows x 12 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "# sys.path.append('../')\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "# TEST\n",
        "test\n",
        "\n",
        "# # PREDICTIONS\n",
        "\n",
        "\n",
        "# # Detect users from training set that are not in test\n",
        "# nb_users = set([pred.uid for pred in pred_nb_list])\n",
        "# lf_users = set([pred.uid for pred in pred_lf_list])\n",
        "# nb_users_in_pred_but_not_in_test = list(nb_users.difference(set(df_test['reviewerID'])))\n",
        "# lf_users_in_pred_but_not_in_test = list(lf_users.difference(set(df_test['reviewerID'])))\n",
        "# assert nb_users_in_pred_but_not_in_test == lf_users_in_pred_but_not_in_test\n",
        "# print(f\"There are {len(lf_users_in_pred_but_not_in_test)} users in the training set that are not in the test set.\")\n",
        "\n",
        "# # Remove these users' predictions for evaluation\n",
        "# ### YOUR CODE HERE ###\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test.info()\n",
        "\n",
        "(len(test['reviewerID'].value_counts()))\n",
        "\n",
        "# train[~train.reviewerID.isin(test.reviewerID)].info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working with 949 users and 47 items. \n",
            "There are 32 users in training set that are not in the test set. \n",
            "Evaluating the systems with 52988 predictions for users in the test split.\n",
            "RMSE for Neighborhood based Collaborative Filtering: 0.6855655559701531\n",
            "RMSE for Latent Factor based Collaborative Filtering: 0.5485685650260251\n"
          ]
        }
      ],
      "source": [
        "#dataframe with users in train that are not in test dataset.\n",
        "re = train[~train.reviewerID.isin(test.reviewerID)]\n",
        "\n",
        "# # Remove these users' predictions for evaluation\n",
        "upd_pred_mbm__df =pred_mbm__df[pred_mbm__df.uid.isin(test.reviewerID)]\n",
        "upd_pred_nbm_df = pred_nbm_df[pred_nbm_df.uid.isin(test.reviewerID)]\n",
        "\n",
        "\n",
        "print('Working with {} users and {} items. '.format(len(test['reviewerID'].value_counts()), len(test['asin'].value_counts())))\n",
        "print('There are {} users in training set that are not in the test set. '.format(len(re['reviewerID'].value_counts())))\n",
        "print('Evaluating the systems with {} predictions for users in the test split.'.format(len(upd_pred_nbm_df)))\n",
        "\n",
        "\n",
        "# determine RMSE of cleaned predictions dataframe\n",
        "#1st must pass df to list of lists\n",
        "\n",
        "rmse_mbm =surprise.accuracy.rmse(upd_pred_mbm__df.values.tolist(), verbose=False)\n",
        "rmse_nbm =surprise.accuracy.rmse(upd_pred_nbm_df.values.tolist(), verbose=False)\n",
        "\n",
        "\n",
        "\n",
        "print('RMSE for Neighborhood based Collaborative Filtering: {}'.format(rmse_nbm) )\n",
        "\n",
        "print('RMSE for Latent Factor based Collaborative Filtering: {}'.format(rmse_mbm) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2\n",
        "Define a general method to get the top-k recommendations for each user. Print the top-k with k={5, 10} recommendations for the user with ID 'ARARUVZ8RUF5T' and its estimated ratings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'upd_pred_nbm_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\franc\\PycharmProjects\\WebScicence_FinalProject\\labs\\session_1\\Session_1.ipynb Cell 79'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franc/PycharmProjects/WebScicence_FinalProject/labs/session_1/Session_1.ipynb#ch0000078?line=4'>5</a>\u001b[0m     df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39muid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mest\u001b[39m\u001b[39m'\u001b[39m], ascending\u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franc/PycharmProjects/WebScicence_FinalProject/labs/session_1/Session_1.ipynb#ch0000078?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m df\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39muid\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mhead(k)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/franc/PycharmProjects/WebScicence_FinalProject/labs/session_1/Session_1.ipynb#ch0000078?line=7'>8</a>\u001b[0m babe \u001b[39m=\u001b[39m top_rec(upd_pred_nbm_df, \u001b[39m10\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franc/PycharmProjects/WebScicence_FinalProject/labs/session_1/Session_1.ipynb#ch0000078?line=9'>10</a>\u001b[0m babe\u001b[39m.\u001b[39mloc[babe[\u001b[39m'\u001b[39m\u001b[39muid\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m name]\n",
            "\u001b[1;31mNameError\u001b[0m: name 'upd_pred_nbm_df' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "name = 'ARARUVZ8RUF5T'\n",
        "item = 'B019FWRG3C'\n",
        "\n",
        "def top_rec(df, k):\n",
        "    df = df.sort_values(by= ['uid', 'est'], ascending= False)\n",
        "    return df.groupby('uid').head(k)\n",
        "\n",
        "babe = top_rec(upd_pred_nbm_df, 10)\n",
        "\n",
        "babe.loc[babe['uid'] == name]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Excercise 3\n",
        "Report Precision@k (P@k), MAP@k and the MRR@k with k={5, 10, 20} averaged across users for both CF systems. When computing precision, we consider as relevant items those with an observed rating >= 4.0 (i.e., those items from the test set with a rating >= 4.0). Reflect on the differences obtained. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Metrics0:\n",
        "\n",
        "  def __init__(self,df,test,k):\n",
        "\n",
        "    self.df = df\n",
        "    self.test = test\n",
        "    self.k = k\n",
        "\n",
        "  def prepare(self):\n",
        "    \"\"\" Join prediction df with test df so the final df has at the ground truth (in this case there's just 1 GT per user)\"\"\"\n",
        "\n",
        "    # upd_pred_nbm_df = top_rec(upd_pred_nbm_df, 10)\n",
        "    # get r_ui column all to zeros\n",
        "    upd_pred_nbm_df = self.df.assign(r_ui=0)\n",
        "\n",
        "    #prepare test df for merging\n",
        "    new_test = self.test[['reviewerID', 'asin', 'overall']]\n",
        "    new_column_list = ['uid', 'iid', 'r_ui']\n",
        "    new_test = new_test.set_axis(new_column_list, axis=1)\n",
        "\n",
        "    #concat predictions df with test df\n",
        "    joint = pd.concat([upd_pred_nbm_df, new_test]).sort_values(by = ['uid', 'iid'])\n",
        "\n",
        "    #make df with just duplicates\n",
        "    duplies = joint[joint.duplicated(subset = ['uid', 'iid'], keep= False)].sort_values(by=['uid', 'r_ui'])\n",
        "\n",
        "    #shift up by 1, so the predictions rows have the real value\n",
        "    duplies['r_ui'] = duplies['r_ui'].shift(-1)\n",
        "\n",
        "    #drop test rows. They no longer matter\n",
        "    no_duplies = duplies[duplies['est'].notna()]\n",
        "\n",
        "\n",
        "    final = pd.concat([joint, no_duplies]).sort_values(by = ['uid', 'iid', 'r_ui']).drop_duplicates(['uid', 'iid'], keep = 'last')#.reset_index(drop=True)\n",
        "    final = final.sort_index(axis = 0)\n",
        "    final = final.sort_values(by= ['uid', 'est'], ascending= False)\n",
        "\n",
        "    #make new column with row index by group\n",
        "    final['group_index'] = final.groupby('uid').cumcount()+1\n",
        "\n",
        "    return final\n",
        "\n",
        "\n",
        "  def get_precision(self):\n",
        "\n",
        "    final = self.prepare()\n",
        "    tt = final.groupby(['uid']).head(self.k)\n",
        "\n",
        "    gg = tt.groupby(['uid']).agg(lambda x: x.ne(0).sum())\n",
        "\n",
        "    return np.mean(gg['r_ui']/self.k)\n",
        "    # return gg\n",
        "\n",
        "\n",
        "  \n",
        "  def hit_rate(self):\n",
        "    \"\"\" Difference from Precision@k is: at the end \n",
        "    -> we have count of non-zeros 'r_ui' per user. (Note: in this exercicise we get AT MOST 1 count per user, because of initial building of test dataset, with just 1 item per user).\n",
        "    -> calculate mean over all users.\n",
        "    In Precision@k:\n",
        "    -> .... same\n",
        "    -> divide the count of non-zeros for a given user by k. \n",
        "    -> caluclate mean over all users  \"\"\"\n",
        "    \n",
        "    final = self.prepare()\n",
        "\n",
        "    # Select only top k rows for each user\n",
        "    tt = final.groupby(['uid']).head(self.k)\n",
        "\n",
        "    # Count number of non-zero elements PER USER. It returns df with many columns (only 'r_ui' matters to us)\n",
        "    gg = tt.groupby(['uid']).agg(lambda x: x.ne(0).sum())\n",
        "\n",
        "    return np.mean(gg['r_ui'])\n",
        "\n",
        "\n",
        "  def get_MRR(self):\n",
        "\n",
        "    final = self.prepare()\n",
        "\n",
        "    final = final.groupby(['uid']).head(self.k)\n",
        "    final = final[final['r_ui'].apply(lambda x: x != 0)]\n",
        "\n",
        "\n",
        "    \n",
        "    return (sum(1/final['group_index']))/ len(self.test)\n",
        "\n",
        "\n",
        "  def get_MAP(self):\n",
        "    \n",
        "    df = self.prepare()\n",
        "\n",
        "    df = df.groupby(['uid']).head(self.k)\n",
        "\n",
        "    #make new column with row index by group\n",
        "    # df['group_index'] = df.groupby('uid').cumcount()\n",
        "\n",
        "    # get rows index that matter to calculate MAP@k\n",
        "    df_new = df[df['r_ui'].apply(lambda x: x != 0)]\n",
        "\n",
        "    return sum(1/(df_new['group_index']))/len(test)\n",
        "\n",
        "    # return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hola = Metrics0(upd_pred_nbm_df, test, 20)\n",
        "\n",
        "hola.get_precision()\n",
        "hola.get_MAP()\n",
        "hola.get_MRR()\n",
        "\n",
        "hola.hit_rate()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "hola = Metrics0(upd_pred_mbm__df, test, 20)\n",
        "\n",
        "hola.get_precision()\n",
        "hola.get_MAP()\n",
        "hola.get_MRR()\n",
        "\n",
        "hola.hit_rate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Excercise 4\n",
        "\n",
        "Based on the top-5, top-10 and top-20 predictions from Exercise 2, compute the systems’ hit rate averaged over the total number of users in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s5yo2Dqjqef"
      },
      "source": [
        "Please, note that this notebook is intended to be run in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkMH-1WClvTd"
      },
      "outputs": [],
      "source": [
        "# # Mount drive and define path to the data folder (from your Google Drive)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# datapath = 'drive/MyDrive/data/amazon_reviews/All_Beauty/'\n",
        "# train_file = 'training.pkl'\n",
        "# test_file = 'test.pkl'\n",
        "# meta_file = 'meta_All_Beauty.json'\n",
        "# uy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "# % wget http://deepyeti.ucsd.edu/jianmo/amazon/sample/meta_All_Beauty.json.gz\n",
        "\n",
        "# http://deepyeti.ucsd.edu/jianmo/amazon/sample/meta_Computers.json.gz\n",
        "\n",
        "# Software\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIYb2Pxi6_Ie"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "Load the [metadata file](https://nijianmo.github.io/amazon/index.html) and discard any item that was not rated by our subset of users (nor in training or test sets). Apply preprocessing (stemming and stopwords removal) to clean up the text from the \"title\". Report the vocabulary size before and after the preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CKbUQSlc65kO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\franc\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\franc\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import regex as re\n",
        "\n",
        "# Load TRAIN and TEST sets \n",
        "\n",
        "# Load the METADATA (ITEMS)\n",
        "\n",
        "# Discard duplicates\n",
        "\n",
        "# Discard items that weren't rated by our subset of users\n",
        "\n",
        "import nltk\n",
        "# nltk.download()\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "from nltk.lm import Vocabulary\n",
        "\n",
        "import itertools\n",
        "import string\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "\n",
        "# <YOUR CODE HERE>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# access items in df 5_core\n",
        "bebe = Data(df)\n",
        "\n",
        "items_train = bebe.get_train()['asin'].unique()\n",
        "items_test = bebe.get_test()['asin'].unique()\n",
        "\n",
        "items = pd.DataFrame(np.concatenate((items_train, items_test)))[0].unique()\n",
        "\n",
        "len(items) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32488"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "meta_df = getDF('meta_All_Beauty.json.gz')\n",
        "\n",
        "meta_df['asin'].nunique()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>tech1</th>\n",
              "      <th>description</th>\n",
              "      <th>fit</th>\n",
              "      <th>title</th>\n",
              "      <th>also_buy</th>\n",
              "      <th>tech2</th>\n",
              "      <th>brand</th>\n",
              "      <th>feature</th>\n",
              "      <th>rank</th>\n",
              "      <th>also_view</th>\n",
              "      <th>details</th>\n",
              "      <th>main_cat</th>\n",
              "      <th>similar_item</th>\n",
              "      <th>date</th>\n",
              "      <th>price</th>\n",
              "      <th>asin</th>\n",
              "      <th>imageURL</th>\n",
              "      <th>imageURLHighRes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[INDICATIONS: Aqua Velva Cooling After Shave E...</td>\n",
              "      <td></td>\n",
              "      <td>Aqua Velva After Shave, Classic Ice Blue, 7 Ounce</td>\n",
              "      <td>[B00J232PCM, B0010V5MKG, B000052Y68, B00KOAIU7...</td>\n",
              "      <td></td>\n",
              "      <td>Aqua Velva</td>\n",
              "      <td>[]</td>\n",
              "      <td>65,003 in Beauty &amp; Personal Care (</td>\n",
              "      <td>[B01I9TIY1U, B07L1PZCS7, B01N12C89Y, B01I9TINT...</td>\n",
              "      <td>{'\n",
              "    Product Dimensions: \n",
              "    ': '3 x 4 x 5 ...</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>B0000530HU</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[&lt;P&gt;&lt;STRONG&gt;Restores Moisture to Dehydrated Ha...</td>\n",
              "      <td></td>\n",
              "      <td>Citre Shine Moisture Burst Shampoo - 16 fl oz</td>\n",
              "      <td>[B07CSVCGZV, B07KMGC13Z, B0793XJ4WW, B01N7U1HB...</td>\n",
              "      <td></td>\n",
              "      <td>Citre Shine</td>\n",
              "      <td>[]</td>\n",
              "      <td>1,693,702 in Beauty &amp; Personal Care (</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'ASIN: ': 'B00006L9LC', 'UPC:': '795827187965...</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>$23.00</td>\n",
              "      <td>B00006L9LC</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[A richly pigmented, micronized powder formula...</td>\n",
              "      <td></td>\n",
              "      <td>NARS Blush, Taj Mahal</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>NARS</td>\n",
              "      <td>[]</td>\n",
              "      <td>505,302 in Beauty &amp; Personal Care (</td>\n",
              "      <td>[B07FVJJ39R, B07JBQZDKB, B07HKVJC7G, B010VWL4E...</td>\n",
              "      <td>{'\n",
              "    Item Weight: \n",
              "    ': '0.16 ounces', 'Sh...</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>$34.50</td>\n",
              "      <td>B00021DJ32</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[Avalon Organics Wrinkle Therapy Cleansing Mil...</td>\n",
              "      <td></td>\n",
              "      <td>Avalon Organics Wrinkle Therapy CoQ10 Cleansin...</td>\n",
              "      <td>[B0014407HC, B001ECQ41M, B00503OFIU, B00015XAQ...</td>\n",
              "      <td></td>\n",
              "      <td>Avalon</td>\n",
              "      <td>[]</td>\n",
              "      <td>141,988 in Beauty &amp;amp; Personal Care (</td>\n",
              "      <td>[B077ZG4C3L, B07DW6ZLFS, B00503OFIU, B07DVZMGL...</td>\n",
              "      <td>{'\n",
              "    Product Dimensions: \n",
              "    ': '2.5 x 1.4 ...</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>$8.27</td>\n",
              "      <td>B0002JHI1I</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>881</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[We bring you this all-natural, luxury Zum bar...</td>\n",
              "      <td></td>\n",
              "      <td>ZUM Zum Bar Anise Lavender, 3 Ounce</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>ZUM</td>\n",
              "      <td>[]</td>\n",
              "      <td>586,248 in Beauty &amp; Personal Care (</td>\n",
              "      <td>[B005CYJN0W, B000DN7BW4, B001EXT5J4, B000DLB26...</td>\n",
              "      <td>{'\n",
              "    Product Dimensions: \n",
              "    ': '0.8 x 2.2 ...</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>$7.76</td>\n",
              "      <td>B0006O10P4</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26610</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[new authentic discontinued]</td>\n",
              "      <td></td>\n",
              "      <td>Ultimate Body Lotion By Michael Kors 3.4oz</td>\n",
              "      <td>[B00R1QRWLG]</td>\n",
              "      <td></td>\n",
              "      <td>Michael Kors</td>\n",
              "      <td>[]</td>\n",
              "      <td>736,956 in Beauty &amp; Personal Care (</td>\n",
              "      <td>[B016AGM1MW, B019AWE8TW]</td>\n",
              "      <td>{'ASIN: ': 'B019LAI4HU', 'UPC:': '682821146329'}</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>B019LAI4HU</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26759</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[Launched by the design house of dolce and gab...</td>\n",
              "      <td></td>\n",
              "      <td>Dolce &amp;amp; Gabbana Compact Parfum, 0.05 Ounce</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>Dolce &amp; Gabbana</td>\n",
              "      <td>[]</td>\n",
              "      <td>863,502 in Beauty &amp; Personal Care (</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'\n",
              "    Product Dimensions: \n",
              "    ': '3.5 x 1.2 ...</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>B019V2KYZS</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28095</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[Colgate Kids Maximum Cavity Protection Pump T...</td>\n",
              "      <td></td>\n",
              "      <td>Colgate Kids Maximum Cavity Protection Pump To...</td>\n",
              "      <td>[B07G35BLNY, B01MR4VL2E, B003XDX68E, B002VA4FY...</td>\n",
              "      <td></td>\n",
              "      <td>Colgate</td>\n",
              "      <td>[&lt;span class=\"a-size-base a-color-secondary\"&gt;\\...</td>\n",
              "      <td>307,236 in Beauty &amp; Personal Care (</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'\n",
              "    Product Dimensions: \n",
              "    ': '2 x 0.9 x ...</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>B01BNEYGQU</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29860</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>Bali Secrets Natural Deodorant - Organic &amp;amp;...</td>\n",
              "      <td>[B07DNGSBSC, B00EOB0042, B077F4NB55, B07GXZYBX...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>152,867 in Beauty &amp; Personal Care (</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'\n",
              "    Product Dimensions: \n",
              "    ': '4.3 x 1.8 ...</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>B01DKQAXC0</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30471</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[the pleasure is all mine. loose the formaliti...</td>\n",
              "      <td></td>\n",
              "      <td>essie Gel Couture Nail Polish</td>\n",
              "      <td>[B01E7UKPWQ, B01E7UKTAE, B01E7UKS7S, B071RKDH9...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>306,101 in Beauty &amp; Personal Care (</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'\n",
              "    Product Dimensions: \n",
              "    ': '1.5 x 1.5 ...</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>$11.25</td>\n",
              "      <td>B01E7UKR38</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>84 rows × 19 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      category tech1                                        description fit  \\\n",
              "50          []        [INDICATIONS: Aqua Velva Cooling After Shave E...       \n",
              "111         []        [<P><STRONG>Restores Moisture to Dehydrated Ha...       \n",
              "293         []        [A richly pigmented, micronized powder formula...       \n",
              "392         []        [Avalon Organics Wrinkle Therapy Cleansing Mil...       \n",
              "881         []        [We bring you this all-natural, luxury Zum bar...       \n",
              "...        ...   ...                                                ...  ..   \n",
              "26610       []                             [new authentic discontinued]       \n",
              "26759       []        [Launched by the design house of dolce and gab...       \n",
              "28095       []        [Colgate Kids Maximum Cavity Protection Pump T...       \n",
              "29860       []                                                       []       \n",
              "30471       []        [the pleasure is all mine. loose the formaliti...       \n",
              "\n",
              "                                                   title  \\\n",
              "50     Aqua Velva After Shave, Classic Ice Blue, 7 Ounce   \n",
              "111        Citre Shine Moisture Burst Shampoo - 16 fl oz   \n",
              "293                                NARS Blush, Taj Mahal   \n",
              "392    Avalon Organics Wrinkle Therapy CoQ10 Cleansin...   \n",
              "881                  ZUM Zum Bar Anise Lavender, 3 Ounce   \n",
              "...                                                  ...   \n",
              "26610         Ultimate Body Lotion By Michael Kors 3.4oz   \n",
              "26759     Dolce &amp; Gabbana Compact Parfum, 0.05 Ounce   \n",
              "28095  Colgate Kids Maximum Cavity Protection Pump To...   \n",
              "29860  Bali Secrets Natural Deodorant - Organic &amp;...   \n",
              "30471                      essie Gel Couture Nail Polish   \n",
              "\n",
              "                                                also_buy tech2  \\\n",
              "50     [B00J232PCM, B0010V5MKG, B000052Y68, B00KOAIU7...         \n",
              "111    [B07CSVCGZV, B07KMGC13Z, B0793XJ4WW, B01N7U1HB...         \n",
              "293                                                   []         \n",
              "392    [B0014407HC, B001ECQ41M, B00503OFIU, B00015XAQ...         \n",
              "881                                                   []         \n",
              "...                                                  ...   ...   \n",
              "26610                                       [B00R1QRWLG]         \n",
              "26759                                                 []         \n",
              "28095  [B07G35BLNY, B01MR4VL2E, B003XDX68E, B002VA4FY...         \n",
              "29860  [B07DNGSBSC, B00EOB0042, B077F4NB55, B07GXZYBX...         \n",
              "30471  [B01E7UKPWQ, B01E7UKTAE, B01E7UKS7S, B071RKDH9...         \n",
              "\n",
              "                 brand                                            feature  \\\n",
              "50          Aqua Velva                                                 []   \n",
              "111        Citre Shine                                                 []   \n",
              "293               NARS                                                 []   \n",
              "392             Avalon                                                 []   \n",
              "881                ZUM                                                 []   \n",
              "...                ...                                                ...   \n",
              "26610     Michael Kors                                                 []   \n",
              "26759  Dolce & Gabbana                                                 []   \n",
              "28095          Colgate  [<span class=\"a-size-base a-color-secondary\">\\...   \n",
              "29860                                                                  []   \n",
              "30471                                                                  []   \n",
              "\n",
              "                                          rank  \\\n",
              "50          65,003 in Beauty & Personal Care (   \n",
              "111      1,693,702 in Beauty & Personal Care (   \n",
              "293        505,302 in Beauty & Personal Care (   \n",
              "392    141,988 in Beauty &amp; Personal Care (   \n",
              "881        586,248 in Beauty & Personal Care (   \n",
              "...                                        ...   \n",
              "26610      736,956 in Beauty & Personal Care (   \n",
              "26759      863,502 in Beauty & Personal Care (   \n",
              "28095      307,236 in Beauty & Personal Care (   \n",
              "29860      152,867 in Beauty & Personal Care (   \n",
              "30471      306,101 in Beauty & Personal Care (   \n",
              "\n",
              "                                               also_view  \\\n",
              "50     [B01I9TIY1U, B07L1PZCS7, B01N12C89Y, B01I9TINT...   \n",
              "111                                                   []   \n",
              "293    [B07FVJJ39R, B07JBQZDKB, B07HKVJC7G, B010VWL4E...   \n",
              "392    [B077ZG4C3L, B07DW6ZLFS, B00503OFIU, B07DVZMGL...   \n",
              "881    [B005CYJN0W, B000DN7BW4, B001EXT5J4, B000DLB26...   \n",
              "...                                                  ...   \n",
              "26610                           [B016AGM1MW, B019AWE8TW]   \n",
              "26759                                                 []   \n",
              "28095                                                 []   \n",
              "29860                                                 []   \n",
              "30471                                                 []   \n",
              "\n",
              "                                                 details    main_cat  \\\n",
              "50     {'\n",
              "    Product Dimensions: \n",
              "    ': '3 x 4 x 5 ...  All Beauty   \n",
              "111    {'ASIN: ': 'B00006L9LC', 'UPC:': '795827187965...  All Beauty   \n",
              "293    {'\n",
              "    Item Weight: \n",
              "    ': '0.16 ounces', 'Sh...  All Beauty   \n",
              "392    {'\n",
              "    Product Dimensions: \n",
              "    ': '2.5 x 1.4 ...  All Beauty   \n",
              "881    {'\n",
              "    Product Dimensions: \n",
              "    ': '0.8 x 2.2 ...  All Beauty   \n",
              "...                                                  ...         ...   \n",
              "26610   {'ASIN: ': 'B019LAI4HU', 'UPC:': '682821146329'}  All Beauty   \n",
              "26759  {'\n",
              "    Product Dimensions: \n",
              "    ': '3.5 x 1.2 ...  All Beauty   \n",
              "28095  {'\n",
              "    Product Dimensions: \n",
              "    ': '2 x 0.9 x ...  All Beauty   \n",
              "29860  {'\n",
              "    Product Dimensions: \n",
              "    ': '4.3 x 1.8 ...  All Beauty   \n",
              "30471  {'\n",
              "    Product Dimensions: \n",
              "    ': '1.5 x 1.5 ...  All Beauty   \n",
              "\n",
              "      similar_item date   price        asin  \\\n",
              "50                               B0000530HU   \n",
              "111                      $23.00  B00006L9LC   \n",
              "293                      $34.50  B00021DJ32   \n",
              "392                       $8.27  B0002JHI1I   \n",
              "881                       $7.76  B0006O10P4   \n",
              "...            ...  ...     ...         ...   \n",
              "26610                            B019LAI4HU   \n",
              "26759                            B019V2KYZS   \n",
              "28095                            B01BNEYGQU   \n",
              "29860                            B01DKQAXC0   \n",
              "30471                    $11.25  B01E7UKR38   \n",
              "\n",
              "                                                imageURL  \\\n",
              "50     [https://images-na.ssl-images-amazon.com/image...   \n",
              "111                                                   []   \n",
              "293    [https://images-na.ssl-images-amazon.com/image...   \n",
              "392    [https://images-na.ssl-images-amazon.com/image...   \n",
              "881    [https://images-na.ssl-images-amazon.com/image...   \n",
              "...                                                  ...   \n",
              "26610                                                 []   \n",
              "26759  [https://images-na.ssl-images-amazon.com/image...   \n",
              "28095  [https://images-na.ssl-images-amazon.com/image...   \n",
              "29860  [https://images-na.ssl-images-amazon.com/image...   \n",
              "30471  [https://images-na.ssl-images-amazon.com/image...   \n",
              "\n",
              "                                         imageURLHighRes  \n",
              "50     [https://images-na.ssl-images-amazon.com/image...  \n",
              "111                                                   []  \n",
              "293    [https://images-na.ssl-images-amazon.com/image...  \n",
              "392    [https://images-na.ssl-images-amazon.com/image...  \n",
              "881    [https://images-na.ssl-images-amazon.com/image...  \n",
              "...                                                  ...  \n",
              "26610                                                 []  \n",
              "26759  [https://images-na.ssl-images-amazon.com/image...  \n",
              "28095  [https://images-na.ssl-images-amazon.com/image...  \n",
              "29860  [https://images-na.ssl-images-amazon.com/image...  \n",
              "30471  [https://images-na.ssl-images-amazon.com/image...  \n",
              "\n",
              "[84 rows x 19 columns]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# df_clean = meta_df[meta_df['overall'].notna()]\n",
        "\n",
        "\n",
        "def clean_meta(df):\n",
        "\n",
        "    df_clean = df[df['asin'].notna()]\n",
        "\n",
        "    df_clean = df.sort_values(by =[ 'asin'])\n",
        "    \n",
        "    # in case i want to check duplicates\n",
        "    # df_clean[df_clean.duplicated(subset = ['asin'], keep = False)]\n",
        "\n",
        "    df_clean = df.drop_duplicates(subset = [ 'asin', ], keep = 'first')\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "beauty_meta_clean = clean_meta(meta_df)\n",
        "\n",
        "# beauty_meta_clean\n",
        "\n",
        "temp = beauty_meta_clean[beauty_meta_clean.asin.isin(items)]\n",
        "(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "545"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "with no tokenization,  5413\n",
            "After removel of punctuation and numbers,  804\n",
            "after removel of stopwords,  711\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "405"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "jhsgdfjsdg 392\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['aqua',\n",
              " 'velva',\n",
              " 'after',\n",
              " 'shave',\n",
              " 'classic',\n",
              " 'ice',\n",
              " 'blue',\n",
              " 'ounce',\n",
              " 'citre',\n",
              " 'shine',\n",
              " 'moisture',\n",
              " 'burst',\n",
              " 'shampoo',\n",
              " 'fl',\n",
              " 'oz',\n",
              " 'nars',\n",
              " 'blush',\n",
              " 'taj',\n",
              " 'mahal',\n",
              " 'avalon',\n",
              " 'organics',\n",
              " 'wrinkle',\n",
              " 'therapy',\n",
              " 'coq',\n",
              " 'cleansing',\n",
              " 'milk',\n",
              " 'oz',\n",
              " 'zum',\n",
              " 'zum',\n",
              " 'bar',\n",
              " 'anise',\n",
              " 'lavender',\n",
              " 'ounce',\n",
              " 'yardley',\n",
              " 'by',\n",
              " 'yardley',\n",
              " 'of',\n",
              " 'london',\n",
              " 'unisexs',\n",
              " 'lay',\n",
              " 'it',\n",
              " 'on',\n",
              " 'thick',\n",
              " 'hand',\n",
              " 'amp',\n",
              " 'foot',\n",
              " 'cream',\n",
              " 'oz',\n",
              " 'fruits',\n",
              " 'amp',\n",
              " 'passion',\n",
              " 'blue',\n",
              " 'refreshing',\n",
              " 'shower',\n",
              " 'gel',\n",
              " 'fl',\n",
              " 'oz',\n",
              " 'waterpik',\n",
              " 'ultra',\n",
              " 'water',\n",
              " 'flosser',\n",
              " 'aqua',\n",
              " 'velva',\n",
              " 'after',\n",
              " 'shave',\n",
              " 'classic',\n",
              " 'ice',\n",
              " 'blue',\n",
              " 'ounce',\n",
              " 'waterpik',\n",
              " 'ultra',\n",
              " 'water',\n",
              " 'flosser',\n",
              " 'fresh',\n",
              " 'eau',\n",
              " 'de',\n",
              " 'parfum',\n",
              " 'sugar',\n",
              " 'lemon',\n",
              " 'oz',\n",
              " 'crest',\n",
              " 'pro',\n",
              " 'health',\n",
              " 'multi',\n",
              " 'protection',\n",
              " 'rinse',\n",
              " 'cool',\n",
              " 'wintergreen',\n",
              " 'fluid',\n",
              " 'ounce',\n",
              " 'philips',\n",
              " 'norelco',\n",
              " 'arcitec',\n",
              " 'men',\n",
              " 's',\n",
              " 'shaving',\n",
              " 'system',\n",
              " 'bonne',\n",
              " 'bell',\n",
              " 'smackers',\n",
              " 'bath',\n",
              " 'and',\n",
              " 'body',\n",
              " 'starburst',\n",
              " 'collection',\n",
              " 'philips',\n",
              " 'sonicare',\n",
              " 'uv',\n",
              " 'sanitizer',\n",
              " 'maggie',\n",
              " 's',\n",
              " 'functional',\n",
              " 'organics',\n",
              " 'raspberry',\n",
              " 'navy',\n",
              " 'forest',\n",
              " 'essie',\n",
              " 'nail',\n",
              " 'polish',\n",
              " 'cuticle',\n",
              " 'care',\n",
              " 'primers',\n",
              " 'and',\n",
              " 'finishers',\n",
              " 'sex',\n",
              " 'in',\n",
              " 'the',\n",
              " 'city',\n",
              " 'kiss',\n",
              " 'by',\n",
              " 'instyle',\n",
              " 'parfums',\n",
              " 'eau',\n",
              " 'de',\n",
              " 'parfum',\n",
              " 'spray',\n",
              " 'oz',\n",
              " 'wiseways',\n",
              " 'herbals',\n",
              " 'witch',\n",
              " 'hazel',\n",
              " 'salve',\n",
              " 'oz',\n",
              " 'helen',\n",
              " 'of',\n",
              " 'troy',\n",
              " 'tangle',\n",
              " 'free',\n",
              " 'hot',\n",
              " 'air',\n",
              " 'brush',\n",
              " 'white',\n",
              " 'inch',\n",
              " 'barrel',\n",
              " 'dr',\n",
              " 'woods',\n",
              " 'pure',\n",
              " 'almond',\n",
              " 'liquid',\n",
              " 'castile',\n",
              " 'soap',\n",
              " 'ounce',\n",
              " 'american',\n",
              " 'crew',\n",
              " 'by',\n",
              " 'american',\n",
              " 'crew',\n",
              " 'classic',\n",
              " 'body',\n",
              " 'wash',\n",
              " 'oz',\n",
              " 'kent',\n",
              " 'the',\n",
              " 'handmade',\n",
              " 'comb',\n",
              " 'fine',\n",
              " 'and',\n",
              " 'coarse',\n",
              " 'toothed',\n",
              " 'pocket',\n",
              " 'comb',\n",
              " 'sawcut',\n",
              " 'r',\n",
              " 't',\n",
              " 'mm',\n",
              " 'urban',\n",
              " 'spa',\n",
              " 'moisturizing',\n",
              " 'booties',\n",
              " 'to',\n",
              " 'keep',\n",
              " 'your',\n",
              " 'feet',\n",
              " 'smooth',\n",
              " 'hydrated',\n",
              " 'and',\n",
              " 'moisturized',\n",
              " 'plantlife',\n",
              " 'sandalwood',\n",
              " 'soap',\n",
              " 'bar',\n",
              " 'ounce',\n",
              " 'ageless',\n",
              " 'ultramax',\n",
              " 'gold',\n",
              " 'capsules',\n",
              " 'count',\n",
              " 'avalon',\n",
              " 'organics',\n",
              " 'vitamin',\n",
              " 'c',\n",
              " 'renewal',\n",
              " 'creme',\n",
              " 'oz',\n",
              " 'maui',\n",
              " 'rain',\n",
              " 'hawaiian',\n",
              " 'perfume',\n",
              " 'whish',\n",
              " 'coconut',\n",
              " 'shaving',\n",
              " 'cream',\n",
              " 'smooth',\n",
              " 'all',\n",
              " 'natural',\n",
              " 'shave',\n",
              " 'cream',\n",
              " 'for',\n",
              " 'men',\n",
              " 'amp',\n",
              " 'women',\n",
              " 'leaves',\n",
              " 'skin',\n",
              " 'so',\n",
              " 'soft',\n",
              " 'shea',\n",
              " 'butter',\n",
              " 'and',\n",
              " 'coconut',\n",
              " 'oil',\n",
              " 'natural',\n",
              " 'and',\n",
              " 'organic',\n",
              " 'skin',\n",
              " 'care',\n",
              " 'oz',\n",
              " 'pump',\n",
              " 'bath',\n",
              " 'amp',\n",
              " 'body',\n",
              " 'works',\n",
              " 'ile',\n",
              " 'de',\n",
              " 'tahiti',\n",
              " 'moana',\n",
              " 'coconut',\n",
              " 'vanille',\n",
              " 'moana',\n",
              " 'body',\n",
              " 'wash',\n",
              " 'with',\n",
              " 'tamanoi',\n",
              " 'oz',\n",
              " 'williams',\n",
              " 'lectric',\n",
              " 'shave',\n",
              " 'ounce',\n",
              " 'fusionbeauty',\n",
              " 'liftfusion',\n",
              " 'face',\n",
              " 'lift',\n",
              " 'organic',\n",
              " 'fiji',\n",
              " 'raw',\n",
              " 'organic',\n",
              " 'coconut',\n",
              " 'oil',\n",
              " 'ounce',\n",
              " 'jars',\n",
              " 'nag',\n",
              " 'champa',\n",
              " 'super',\n",
              " 'hit',\n",
              " 'cones',\n",
              " 'cones',\n",
              " 'box',\n",
              " 'axe',\n",
              " 'shower',\n",
              " 'tool',\n",
              " 'detailer',\n",
              " 'ea',\n",
              " 'pack',\n",
              " 'of',\n",
              " 'clubman',\n",
              " 'lustray',\n",
              " 'blue',\n",
              " 'spice',\n",
              " 'after',\n",
              " 'shave',\n",
              " 'fluid',\n",
              " 'ounce',\n",
              " 'clean',\n",
              " 'amp',\n",
              " 'clear',\n",
              " 'deep',\n",
              " 'action',\n",
              " 'cream',\n",
              " 'facial',\n",
              " 'cleanser',\n",
              " 'for',\n",
              " 'sensitive',\n",
              " 'skin',\n",
              " 'gentle',\n",
              " 'daily',\n",
              " 'face',\n",
              " 'wash',\n",
              " 'with',\n",
              " 'oil',\n",
              " 'free',\n",
              " 'oz',\n",
              " 'pack',\n",
              " 'of',\n",
              " 'colgate',\n",
              " 'fluoride',\n",
              " 'toothpaste',\n",
              " 'strawberry',\n",
              " 'smash',\n",
              " 'liquid',\n",
              " 'gel',\n",
              " 'oz',\n",
              " 'pack',\n",
              " 'of',\n",
              " 'oral',\n",
              " 'b',\n",
              " 'glide',\n",
              " 'pro',\n",
              " 'health',\n",
              " 'dental',\n",
              " 'floss',\n",
              " 'original',\n",
              " 'floss',\n",
              " 'm',\n",
              " 'pack',\n",
              " 'of',\n",
              " 'pre',\n",
              " 'de',\n",
              " 'provence',\n",
              " 'maison',\n",
              " 'french',\n",
              " 'dried',\n",
              " 'lavender',\n",
              " 'blossoms',\n",
              " 'for',\n",
              " 'fragrance',\n",
              " 'avalon',\n",
              " 'grapefruit',\n",
              " 'and',\n",
              " 'geranium',\n",
              " 'smoothing',\n",
              " 'shampoo',\n",
              " 'ounce',\n",
              " 'astra',\n",
              " 'platinum',\n",
              " 'double',\n",
              " 'edge',\n",
              " 'safety',\n",
              " 'razor',\n",
              " 'blades',\n",
              " 'blades',\n",
              " 'x',\n",
              " 'urban',\n",
              " 'spa',\n",
              " 'natural',\n",
              " 'bamboo',\n",
              " 'and',\n",
              " 'jute',\n",
              " 'bath',\n",
              " 'mitt',\n",
              " 'kate',\n",
              " 'somerville',\n",
              " 'nbsp',\n",
              " 'exfolikate',\n",
              " 'intensive',\n",
              " 'nbsp',\n",
              " 'exfoliating',\n",
              " 'treatment',\n",
              " 'nbsp',\n",
              " 'fl',\n",
              " 'oz',\n",
              " 'luxury',\n",
              " 'size',\n",
              " 'bvlgari',\n",
              " 'white',\n",
              " 'by',\n",
              " 'bvlgari',\n",
              " 'for',\n",
              " 'men',\n",
              " 'and',\n",
              " 'women',\n",
              " 'shampoo',\n",
              " 'oz',\n",
              " 'folicure',\n",
              " 'shampoo',\n",
              " 'extra',\n",
              " 'body',\n",
              " 'oz',\n",
              " 'pack',\n",
              " 'with',\n",
              " 'free',\n",
              " 'nail',\n",
              " 'file',\n",
              " 'aquaphor',\n",
              " 'healing',\n",
              " 'ointment',\n",
              " 'advanced',\n",
              " 'therapy',\n",
              " 'skin',\n",
              " 'protectant',\n",
              " 'ounce',\n",
              " 'pack',\n",
              " 'may',\n",
              " 'vary',\n",
              " 'nars',\n",
              " 'blush',\n",
              " 'gaiety',\n",
              " 'dark',\n",
              " 'and',\n",
              " 'lovely',\n",
              " 'beautiful',\n",
              " 'beginnings',\n",
              " 'shampoo',\n",
              " 'fluid',\n",
              " 'ounce',\n",
              " 'revitalash',\n",
              " 'by',\n",
              " 'revitalash',\n",
              " 'revitalash',\n",
              " 'advanced',\n",
              " 'eyelash',\n",
              " 'conditioner',\n",
              " 'ml',\n",
              " 'oz',\n",
              " 'colgate',\n",
              " 'enamel',\n",
              " 'health',\n",
              " 'mouthwash',\n",
              " 'andalou',\n",
              " 'naturals',\n",
              " 'clementine',\n",
              " 'c',\n",
              " 'illuminating',\n",
              " 'toner',\n",
              " 'fl',\n",
              " 'oz',\n",
              " 'facial',\n",
              " 'toner',\n",
              " 'helps',\n",
              " 'hydrate',\n",
              " 'amp',\n",
              " 'balance',\n",
              " 'skin',\n",
              " 'ph',\n",
              " 'for',\n",
              " 'clear',\n",
              " 'bright',\n",
              " 'skin',\n",
              " 'hammam',\n",
              " 'el',\n",
              " 'hana',\n",
              " 'argan',\n",
              " 'therapy',\n",
              " 'egyptian',\n",
              " 'white',\n",
              " 'musk',\n",
              " 'body',\n",
              " 'lotion',\n",
              " 'fl',\n",
              " 'oz',\n",
              " 'from',\n",
              " 'turkey',\n",
              " 'mad',\n",
              " 'hippie',\n",
              " 'face',\n",
              " 'cream',\n",
              " 'with',\n",
              " 'anti',\n",
              " 'wrinkle',\n",
              " 'peptide',\n",
              " 'complex',\n",
              " 'ounces',\n",
              " 'pantene',\n",
              " 'pro',\n",
              " 'v',\n",
              " 'ultimate',\n",
              " 'in',\n",
              " 'shampoo',\n",
              " 'and',\n",
              " 'conditioner',\n",
              " 'fl',\n",
              " 'oz',\n",
              " 'packaging',\n",
              " 'may',\n",
              " 'vary',\n",
              " 'jhirmack',\n",
              " 'silver',\n",
              " 'plus',\n",
              " 'shampoo',\n",
              " 'ageless',\n",
              " 'oz',\n",
              " 'pack',\n",
              " 'of',\n",
              " 'victoria',\n",
              " 's',\n",
              " 'secret',\n",
              " 'dream',\n",
              " 'angels',\n",
              " 'heavenly',\n",
              " 'angel',\n",
              " 'wash',\n",
              " 'oz',\n",
              " 'crest',\n",
              " 'oral',\n",
              " 'b',\n",
              " 'professional',\n",
              " 'gingivitis',\n",
              " 'kit',\n",
              " 'count',\n",
              " 'crest',\n",
              " 'pro',\n",
              " 'health',\n",
              " 'for',\n",
              " 'life',\n",
              " 'cpc',\n",
              " 'antigingivitis',\n",
              " 'antiplaque',\n",
              " 'smooth',\n",
              " 'mint',\n",
              " 'rinse',\n",
              " 'fl',\n",
              " 'oz',\n",
              " 'toni',\n",
              " 'amp',\n",
              " 'guy',\n",
              " 'glamour',\n",
              " 'volume',\n",
              " 'plumping',\n",
              " 'whip',\n",
              " 'fluid',\n",
              " 'ounce',\n",
              " 'fekkai',\n",
              " 'full',\n",
              " 'blown',\n",
              " 'aerosol',\n",
              " 'foam',\n",
              " 'cond',\n",
              " 'us',\n",
              " 'oz',\n",
              " 'fluid',\n",
              " 'ounce',\n",
              " 'home',\n",
              " 'health',\n",
              " 'evercln',\n",
              " 'face',\n",
              " 'cream',\n",
              " 'fluid',\n",
              " 'ounce',\n",
              " 'eo',\n",
              " 'shower',\n",
              " 'gel',\n",
              " 'grapefruit',\n",
              " 'amp',\n",
              " 'mint',\n",
              " 'oz',\n",
              " 'spongelle',\n",
              " 'wild',\n",
              " 'flower',\n",
              " 'uses',\n",
              " 'body',\n",
              " 'wash',\n",
              " 'buffer',\n",
              " 'french',\n",
              " 'lavender',\n",
              " 'quot',\n",
              " 'x',\n",
              " 'quot',\n",
              " 'caress',\n",
              " 'body',\n",
              " 'wash',\n",
              " 'sheer',\n",
              " 'twilight',\n",
              " 'oz',\n",
              " 'pk',\n",
              " 'crest',\n",
              " 'sensi',\n",
              " 'stop',\n",
              " 'strips',\n",
              " 'count',\n",
              " 'rimmel',\n",
              " 'provocalips',\n",
              " 'hr',\n",
              " 'kissproof',\n",
              " 'lipstick',\n",
              " 'kiss',\n",
              " 'fatal',\n",
              " 'fluid',\n",
              " 'ounce',\n",
              " 'peter',\n",
              " 'lamas',\n",
              " 'wheatgrass',\n",
              " 'purifying',\n",
              " 'shampoo',\n",
              " 'and',\n",
              " 'conditioner',\n",
              " 'set',\n",
              " 'fluid',\n",
              " 'ounce',\n",
              " 'each',\n",
              " 'pantene',\n",
              " 'pro',\n",
              " 'v',\n",
              " 'volume',\n",
              " 'conditioner',\n",
              " 'fluid',\n",
              " 'ounce',\n",
              " 'product',\n",
              " 'size',\n",
              " 'may',\n",
              " 'vary',\n",
              " 'oznaturals',\n",
              " 'anti',\n",
              " 'aging',\n",
              " 'retinol',\n",
              " 'serum',\n",
              " 'the',\n",
              " 'most',\n",
              " 'effective',\n",
              " 'anti',\n",
              " 'wrinkle',\n",
              " 'serum',\n",
              " 'contains',\n",
              " 'professional',\n",
              " 'strength',\n",
              " 'retinol',\n",
              " 'astaxanthin',\n",
              " 'vitamin',\n",
              " 'e',\n",
              " 'get',\n",
              " 'the',\n",
              " 'dramatic',\n",
              " 'youthful',\n",
              " 'results',\n",
              " 'you',\n",
              " 'rsquo',\n",
              " 've',\n",
              " 'been',\n",
              " 'looking',\n",
              " 'for',\n",
              " 'theorie',\n",
              " 'argan',\n",
              " 'oil',\n",
              " 'ultimate',\n",
              " 'reform',\n",
              " 'shampoo',\n",
              " 'amp',\n",
              " 'conditioner',\n",
              " 'fl',\n",
              " 'oz',\n",
              " 'each',\n",
              " 'sexy',\n",
              " 'straight',\n",
              " 'smooth',\n",
              " 'and',\n",
              " 'seal',\n",
              " 'hair',\n",
              " 'spray',\n",
              " 'ounce',\n",
              " 'pack',\n",
              " 'of',\n",
              " 'lucia',\n",
              " 'flowers',\n",
              " 'print',\n",
              " 'brown',\n",
              " 'large',\n",
              " 'hair',\n",
              " 'clip',\n",
              " 'clamp',\n",
              " 'pack',\n",
              " 'of',\n",
              " 'l',\n",
              " 'a',\n",
              " 'colors',\n",
              " 'mineral',\n",
              " 'pressed',\n",
              " 'powder',\n",
              " 'mp',\n",
              " 'quot',\n",
              " 'creamy',\n",
              " 'natural',\n",
              " 'quot',\n",
              " 'naturelle',\n",
              " 'hypo',\n",
              " 'allergenic',\n",
              " 'styling',\n",
              " 'gel',\n",
              " 'pre',\n",
              " 'de',\n",
              " 'provence',\n",
              " 'artisanal',\n",
              " 'french',\n",
              " 'soap',\n",
              " 'bar',\n",
              " 'enriched',\n",
              " 'with',\n",
              " 'shea',\n",
              " 'butter',\n",
              " 'quad',\n",
              " 'milled',\n",
              " 'for',\n",
              " 'a',\n",
              " 'smooth',\n",
              " 'amp',\n",
              " 'rich',\n",
              " 'lather',\n",
              " 'grams',\n",
              " 'raspberry',\n",
              " 'dove',\n",
              " 'men',\n",
              " 'care',\n",
              " 'deep',\n",
              " 'clean',\n",
              " 'body',\n",
              " 'face',\n",
              " 'bar',\n",
              " 'ounce',\n",
              " 'count',\n",
              " 'pack',\n",
              " 'of',\n",
              " 'hellip',\n",
              " 'lectric',\n",
              " 'shave',\n",
              " 'pre',\n",
              " 'shave',\n",
              " 'original',\n",
              " 'oz',\n",
              " 'pre',\n",
              " 'de',\n",
              " 'provence',\n",
              " 'maison',\n",
              " 'french',\n",
              " 'lavender',\n",
              " 'bath',\n",
              " 'amp',\n",
              " 'shower',\n",
              " 'gel',\n",
              " 'ultimate',\n",
              " 'body',\n",
              " 'lotion',\n",
              " 'by',\n",
              " 'michael',\n",
              " 'kors',\n",
              " 'oz',\n",
              " 'dolce',\n",
              " 'amp',\n",
              " 'gabbana',\n",
              " 'compact',\n",
              " 'parfum',\n",
              " 'ounce',\n",
              " 'colgate',\n",
              " 'kids',\n",
              " 'maximum',\n",
              " 'cavity',\n",
              " 'protection',\n",
              " 'pump',\n",
              " 'toothpaste',\n",
              " 'ounce',\n",
              " 'pack',\n",
              " 'bali',\n",
              " 'secrets',\n",
              " 'natural',\n",
              " 'deodorant',\n",
              " 'organic',\n",
              " 'amp',\n",
              " 'vegan',\n",
              " 'for',\n",
              " 'women',\n",
              " 'amp',\n",
              " 'men',\n",
              " 'all',\n",
              " 'day',\n",
              " 'fresh',\n",
              " 'strong',\n",
              " 'amp',\n",
              " 'reliable',\n",
              " 'protection',\n",
              " 'fl',\n",
              " 'oz',\n",
              " 'ml',\n",
              " 'scent',\n",
              " 'sandalwood',\n",
              " 'essie',\n",
              " 'gel',\n",
              " 'couture',\n",
              " 'nail',\n",
              " 'polish']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "########## needed files #############\n",
        "\n",
        "my_stopwords = open(\"my_stopwords.txt\", \"r\").read().splitlines()\n",
        "# len(my_stopwords)\n",
        "#http://members.unine.ch/jacques.savoy/clef/ 571 stopwords\n",
        "\n",
        "\n",
        "############### No Preprocessing ##################\n",
        "\n",
        "# join all sentences from 'title' into one big sentence\n",
        "text1 = ' '.join(temp['title'].astype(str))\n",
        "\n",
        "# tokenize it. separate words\n",
        "text1 = (word_tokenize(text1))\n",
        "\n",
        "# turn into DataFrame and drop duplicate words. Finally make it a list again\n",
        "no_pre_vocab = (pd.DataFrame(text1)[0].drop_duplicates())\n",
        "\n",
        "len(no_pre_vocab)\n",
        "\n",
        "\n",
        "\n",
        "################ Preprocessing ##################\n",
        "\n",
        "# Must break hyphenized words!!!! \n",
        "text2 = ' '.join(temp['title'].astype(str)).lower()\n",
        "print('with no tokenization, ', len(text2))\n",
        "\n",
        "# remove words that are numbers, punctuations or words with mixture of both\n",
        "tre = \"\".join((char if char.isalpha() else \" \") for char in text2).split()\n",
        "print('After removel of punctuation and numbers, ',len(tre))\n",
        "\n",
        "#remove stop words\n",
        "tokens_wo_stopwords1 = [t for t in tre if t not in my_stopwords]\n",
        "print('after removel of stopwords, ', len(tokens_wo_stopwords1))\n",
        "toy1 = (list(pd.DataFrame(tokens_wo_stopwords1)[0].drop_duplicates()))\n",
        "len(toy1)\n",
        "\n",
        "#stemm it\n",
        "porter = PorterStemmer()\n",
        "\n",
        "stemmed1 = [porter.stem(word) for word in tokens_wo_stopwords1]\n",
        "\n",
        "#remove duplicates\n",
        "vocab = (list(pd.DataFrame(stemmed1)[0].drop_duplicates()))\n",
        "\n",
        "print('jhsgdfjsdg', len(vocab))\n",
        "\n",
        "\n",
        "tre\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Blr1jgoHLbFU"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "Representation in vector spaces.\n",
        "\n",
        "### 2.1\n",
        "\n",
        "Represent all the products from Exercise 1 in a TF-IDF space. Interpret the meaning of the TF-IDF matrix dimensions.\n",
        "\n",
        "Tip: You may use the library [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus_train = list(temp['title'].astype(str))\n",
        "\n",
        "# (corpus_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 409,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>tech1</th>\n",
              "      <th>description</th>\n",
              "      <th>fit</th>\n",
              "      <th>title</th>\n",
              "      <th>also_buy</th>\n",
              "      <th>tech2</th>\n",
              "      <th>brand</th>\n",
              "      <th>feature</th>\n",
              "      <th>rank</th>\n",
              "      <th>also_view</th>\n",
              "      <th>details</th>\n",
              "      <th>main_cat</th>\n",
              "      <th>similar_item</th>\n",
              "      <th>date</th>\n",
              "      <th>price</th>\n",
              "      <th>asin</th>\n",
              "      <th>imageURL</th>\n",
              "      <th>imageURLHighRes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[INDICATIONS: Aqua Velva Cooling After Shave E...</td>\n",
              "      <td></td>\n",
              "      <td>Aqua Velva After Shave, Classic Ice Blue, 7 Ounce</td>\n",
              "      <td>[B00J232PCM, B0010V5MKG, B000052Y68, B00KOAIU7...</td>\n",
              "      <td></td>\n",
              "      <td>Aqua Velva</td>\n",
              "      <td>[]</td>\n",
              "      <td>65,003 in Beauty &amp; Personal Care (</td>\n",
              "      <td>[B01I9TIY1U, B07L1PZCS7, B01N12C89Y, B01I9TINT...</td>\n",
              "      <td>{'\n",
              "    Product Dimensions: \n",
              "    ': '3 x 4 x 5 ...</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>B0000530HU</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[&lt;P&gt;&lt;STRONG&gt;Restores Moisture to Dehydrated Ha...</td>\n",
              "      <td></td>\n",
              "      <td>Citre Shine Moisture Burst Shampoo - 16 fl oz</td>\n",
              "      <td>[B07CSVCGZV, B07KMGC13Z, B0793XJ4WW, B01N7U1HB...</td>\n",
              "      <td></td>\n",
              "      <td>Citre Shine</td>\n",
              "      <td>[]</td>\n",
              "      <td>1,693,702 in Beauty &amp; Personal Care (</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'ASIN: ': 'B00006L9LC', 'UPC:': '795827187965...</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>$23.00</td>\n",
              "      <td>B00006L9LC</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[A richly pigmented, micronized powder formula...</td>\n",
              "      <td></td>\n",
              "      <td>NARS Blush, Taj Mahal</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>NARS</td>\n",
              "      <td>[]</td>\n",
              "      <td>505,302 in Beauty &amp; Personal Care (</td>\n",
              "      <td>[B07FVJJ39R, B07JBQZDKB, B07HKVJC7G, B010VWL4E...</td>\n",
              "      <td>{'\n",
              "    Item Weight: \n",
              "    ': '0.16 ounces', 'Sh...</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>$34.50</td>\n",
              "      <td>B00021DJ32</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    category tech1                                        description fit  \\\n",
              "50        []        [INDICATIONS: Aqua Velva Cooling After Shave E...       \n",
              "111       []        [<P><STRONG>Restores Moisture to Dehydrated Ha...       \n",
              "293       []        [A richly pigmented, micronized powder formula...       \n",
              "\n",
              "                                                 title  \\\n",
              "50   Aqua Velva After Shave, Classic Ice Blue, 7 Ounce   \n",
              "111      Citre Shine Moisture Burst Shampoo - 16 fl oz   \n",
              "293                              NARS Blush, Taj Mahal   \n",
              "\n",
              "                                              also_buy tech2        brand  \\\n",
              "50   [B00J232PCM, B0010V5MKG, B000052Y68, B00KOAIU7...         Aqua Velva   \n",
              "111  [B07CSVCGZV, B07KMGC13Z, B0793XJ4WW, B01N7U1HB...        Citre Shine   \n",
              "293                                                 []               NARS   \n",
              "\n",
              "    feature                                   rank  \\\n",
              "50       []     65,003 in Beauty & Personal Care (   \n",
              "111      []  1,693,702 in Beauty & Personal Care (   \n",
              "293      []    505,302 in Beauty & Personal Care (   \n",
              "\n",
              "                                             also_view  \\\n",
              "50   [B01I9TIY1U, B07L1PZCS7, B01N12C89Y, B01I9TINT...   \n",
              "111                                                 []   \n",
              "293  [B07FVJJ39R, B07JBQZDKB, B07HKVJC7G, B010VWL4E...   \n",
              "\n",
              "                                               details    main_cat  \\\n",
              "50   {'\n",
              "    Product Dimensions: \n",
              "    ': '3 x 4 x 5 ...  All Beauty   \n",
              "111  {'ASIN: ': 'B00006L9LC', 'UPC:': '795827187965...  All Beauty   \n",
              "293  {'\n",
              "    Item Weight: \n",
              "    ': '0.16 ounces', 'Sh...  All Beauty   \n",
              "\n",
              "    similar_item date   price        asin  \\\n",
              "50                             B0000530HU   \n",
              "111                    $23.00  B00006L9LC   \n",
              "293                    $34.50  B00021DJ32   \n",
              "\n",
              "                                              imageURL  \\\n",
              "50   [https://images-na.ssl-images-amazon.com/image...   \n",
              "111                                                 []   \n",
              "293  [https://images-na.ssl-images-amazon.com/image...   \n",
              "\n",
              "                                       imageURLHighRes  \n",
              "50   [https://images-na.ssl-images-amazon.com/image...  \n",
              "111                                                 []  \n",
              "293  [https://images-na.ssl-images-amazon.com/image...  "
            ]
          },
          "execution_count": 409,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temp.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\franc\\miniconda3\\envs\\snowflakes\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "['action',\n",
              " 'advanced',\n",
              " 'aerosol',\n",
              " 'ageless',\n",
              " 'aging',\n",
              " 'air',\n",
              " 'allergenic',\n",
              " 'almond',\n",
              " 'american',\n",
              " 'amp',\n",
              " 'andalou',\n",
              " 'angel',\n",
              " 'angels',\n",
              " 'anise',\n",
              " 'anti',\n",
              " 'antigingivitis',\n",
              " 'antiplaque',\n",
              " 'aqua',\n",
              " 'aquaphor',\n",
              " 'arcitec',\n",
              " 'argan',\n",
              " 'artisanal',\n",
              " 'astaxanthin',\n",
              " 'astra',\n",
              " 'avalon',\n",
              " 'axe',\n",
              " 'balance',\n",
              " 'bali',\n",
              " 'bamboo',\n",
              " 'bar',\n",
              " 'barrel',\n",
              " 'bath',\n",
              " 'beautiful',\n",
              " 'beginnings',\n",
              " 'bell',\n",
              " 'blades',\n",
              " 'blossoms',\n",
              " 'blown',\n",
              " 'blue',\n",
              " 'blush',\n",
              " 'body',\n",
              " 'bonne',\n",
              " 'booties',\n",
              " 'box',\n",
              " 'bright',\n",
              " 'brown',\n",
              " 'brush',\n",
              " 'buffer',\n",
              " 'burst',\n",
              " 'butter',\n",
              " 'bvlgari',\n",
              " 'capsules',\n",
              " 'care',\n",
              " 'caress',\n",
              " 'castile',\n",
              " 'cavity',\n",
              " 'champa',\n",
              " 'citre',\n",
              " 'city',\n",
              " 'clamp',\n",
              " 'classic',\n",
              " 'clean',\n",
              " 'cleanser',\n",
              " 'cleansing',\n",
              " 'clear',\n",
              " 'clementine',\n",
              " 'clip',\n",
              " 'clubman',\n",
              " 'coarse',\n",
              " 'coconut',\n",
              " 'colgate',\n",
              " 'collection',\n",
              " 'colors',\n",
              " 'comb',\n",
              " 'compact',\n",
              " 'complex',\n",
              " 'cond',\n",
              " 'conditioner',\n",
              " 'cones',\n",
              " 'cool',\n",
              " 'count',\n",
              " 'couture',\n",
              " 'cpc',\n",
              " 'cream',\n",
              " 'creamy',\n",
              " 'creme',\n",
              " 'crest',\n",
              " 'crew',\n",
              " 'cuticle',\n",
              " 'daily',\n",
              " 'dark',\n",
              " 'day',\n",
              " 'de',\n",
              " 'deep',\n",
              " 'dental',\n",
              " 'deodorant',\n",
              " 'detailer',\n",
              " 'dolce',\n",
              " 'double',\n",
              " 'dove',\n",
              " 'dr',\n",
              " 'dramatic',\n",
              " 'dream',\n",
              " 'dried',\n",
              " 'ea',\n",
              " 'eau',\n",
              " 'edge',\n",
              " 'effective',\n",
              " 'egyptian',\n",
              " 'el',\n",
              " 'enamel',\n",
              " 'enriched',\n",
              " 'eo',\n",
              " 'essie',\n",
              " 'evercln',\n",
              " 'exfoliating',\n",
              " 'exfolikate',\n",
              " 'extra',\n",
              " 'eyelash',\n",
              " 'face',\n",
              " 'facial',\n",
              " 'fatal',\n",
              " 'feet',\n",
              " 'fekkai',\n",
              " 'fiji',\n",
              " 'file',\n",
              " 'fine',\n",
              " 'finishers',\n",
              " 'fl',\n",
              " 'floss',\n",
              " 'flosser',\n",
              " 'flower',\n",
              " 'flowers',\n",
              " 'fluid',\n",
              " 'fluoride',\n",
              " 'foam',\n",
              " 'folicure',\n",
              " 'foot',\n",
              " 'forest',\n",
              " 'fragrance',\n",
              " 'free',\n",
              " 'french',\n",
              " 'fresh',\n",
              " 'fruits',\n",
              " 'full',\n",
              " 'functional',\n",
              " 'fusionbeauty',\n",
              " 'gabbana',\n",
              " 'gaiety',\n",
              " 'gel',\n",
              " 'gentle',\n",
              " 'geranium',\n",
              " 'gingivitis',\n",
              " 'glamour',\n",
              " 'glide',\n",
              " 'gold',\n",
              " 'grams',\n",
              " 'grapefruit',\n",
              " 'guy',\n",
              " 'hair',\n",
              " 'hammam',\n",
              " 'hana',\n",
              " 'hand',\n",
              " 'handmade',\n",
              " 'hawaiian',\n",
              " 'hazel',\n",
              " 'healing',\n",
              " 'health',\n",
              " 'heavenly',\n",
              " 'helen',\n",
              " 'hellip',\n",
              " 'helps',\n",
              " 'herbals',\n",
              " 'hippie',\n",
              " 'hit',\n",
              " 'home',\n",
              " 'hot',\n",
              " 'hydrate',\n",
              " 'hydrated',\n",
              " 'hypo',\n",
              " 'ice',\n",
              " 'ile',\n",
              " 'illuminating',\n",
              " 'inch',\n",
              " 'instyle',\n",
              " 'intensive',\n",
              " 'jars',\n",
              " 'jhirmack',\n",
              " 'jute',\n",
              " 'kate',\n",
              " 'kent',\n",
              " 'kids',\n",
              " 'kiss',\n",
              " 'kissproof',\n",
              " 'kit',\n",
              " 'kors',\n",
              " 'lamas',\n",
              " 'large',\n",
              " 'lather',\n",
              " 'lavender',\n",
              " 'lay',\n",
              " 'leaves',\n",
              " 'lectric',\n",
              " 'lemon',\n",
              " 'life',\n",
              " 'lift',\n",
              " 'liftfusion',\n",
              " 'lipstick',\n",
              " 'liquid',\n",
              " 'london',\n",
              " 'lotion',\n",
              " 'lovely',\n",
              " 'lucia',\n",
              " 'lustray',\n",
              " 'luxury',\n",
              " 'mad',\n",
              " 'maggie',\n",
              " 'mahal',\n",
              " 'maison',\n",
              " 'maui',\n",
              " 'maximum',\n",
              " 'men',\n",
              " 'michael',\n",
              " 'milk',\n",
              " 'milled',\n",
              " 'mineral',\n",
              " 'mint',\n",
              " 'mitt',\n",
              " 'ml',\n",
              " 'mm',\n",
              " 'moana',\n",
              " 'moisture',\n",
              " 'moisturized',\n",
              " 'moisturizing',\n",
              " 'mouthwash',\n",
              " 'multi',\n",
              " 'musk',\n",
              " 'nag',\n",
              " 'nail',\n",
              " 'nars',\n",
              " 'natural',\n",
              " 'naturals',\n",
              " 'naturelle',\n",
              " 'navy',\n",
              " 'nbsp',\n",
              " 'norelco',\n",
              " 'oil',\n",
              " 'ointment',\n",
              " 'oral',\n",
              " 'organic',\n",
              " 'organics',\n",
              " 'original',\n",
              " 'ounce',\n",
              " 'ounces',\n",
              " 'oz',\n",
              " 'oznaturals',\n",
              " 'pack',\n",
              " 'packaging',\n",
              " 'pantene',\n",
              " 'parfum',\n",
              " 'parfums',\n",
              " 'passion',\n",
              " 'peptide',\n",
              " 'perfume',\n",
              " 'peter',\n",
              " 'ph',\n",
              " 'philips',\n",
              " 'pk',\n",
              " 'plantlife',\n",
              " 'platinum',\n",
              " 'plumping',\n",
              " 'pocket',\n",
              " 'polish',\n",
              " 'powder',\n",
              " 'pre',\n",
              " 'pressed',\n",
              " 'primers',\n",
              " 'print',\n",
              " 'pro',\n",
              " 'product',\n",
              " 'professional',\n",
              " 'protectant',\n",
              " 'protection',\n",
              " 'provence',\n",
              " 'provocalips',\n",
              " 'pump',\n",
              " 'pure',\n",
              " 'purifying',\n",
              " 'quad',\n",
              " 'quot',\n",
              " 'rain',\n",
              " 'raspberry',\n",
              " 'raw',\n",
              " 'razor',\n",
              " 'reform',\n",
              " 'refreshing',\n",
              " 'reliable',\n",
              " 'renewal',\n",
              " 'results',\n",
              " 'retinol',\n",
              " 'revitalash',\n",
              " 'rich',\n",
              " 'rimmel',\n",
              " 'rinse',\n",
              " 'rsquo',\n",
              " 'safety',\n",
              " 'salve',\n",
              " 'sandalwood',\n",
              " 'sanitizer',\n",
              " 'sawcut',\n",
              " 'scent',\n",
              " 'seal',\n",
              " 'secret',\n",
              " 'secrets',\n",
              " 'sensi',\n",
              " 'sensitive',\n",
              " 'serum',\n",
              " 'set',\n",
              " 'sex',\n",
              " 'sexy',\n",
              " 'shampoo',\n",
              " 'shave',\n",
              " 'shaving',\n",
              " 'shea',\n",
              " 'sheer',\n",
              " 'shine',\n",
              " 'shower',\n",
              " 'silver',\n",
              " 'size',\n",
              " 'skin',\n",
              " 'smackers',\n",
              " 'smash',\n",
              " 'smooth',\n",
              " 'smoothing',\n",
              " 'soap',\n",
              " 'soft',\n",
              " 'somerville',\n",
              " 'sonicare',\n",
              " 'spa',\n",
              " 'spice',\n",
              " 'spongelle',\n",
              " 'spray',\n",
              " 'starburst',\n",
              " 'stop',\n",
              " 'straight',\n",
              " 'strawberry',\n",
              " 'strength',\n",
              " 'strips',\n",
              " 'strong',\n",
              " 'styling',\n",
              " 'sugar',\n",
              " 'super',\n",
              " 'system',\n",
              " 'tahiti',\n",
              " 'taj',\n",
              " 'tamanoi',\n",
              " 'tangle',\n",
              " 'theorie',\n",
              " 'therapy',\n",
              " 'thick',\n",
              " 'toner',\n",
              " 'toni',\n",
              " 'tool',\n",
              " 'toothed',\n",
              " 'toothpaste',\n",
              " 'treatment',\n",
              " 'troy',\n",
              " 'turkey',\n",
              " 'twilight',\n",
              " 'ultimate',\n",
              " 'ultra',\n",
              " 'ultramax',\n",
              " 'unisexs',\n",
              " 'urban',\n",
              " 'uv',\n",
              " 'vanille',\n",
              " 'vary',\n",
              " 've',\n",
              " 'vegan',\n",
              " 'velva',\n",
              " 'victoria',\n",
              " 'vitamin',\n",
              " 'volume',\n",
              " 'wash',\n",
              " 'water',\n",
              " 'waterpik',\n",
              " 'wheatgrass',\n",
              " 'whip',\n",
              " 'whish',\n",
              " 'white',\n",
              " 'wild',\n",
              " 'williams',\n",
              " 'wintergreen',\n",
              " 'wiseways',\n",
              " 'witch',\n",
              " 'women',\n",
              " 'woods',\n",
              " 'works',\n",
              " 'wrinkle',\n",
              " 'yardley',\n",
              " 'youthful',\n",
              " 'zum']"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "ds =  temp.reset_index() #pd.read_csv(\"test1.csv\") #you can plug in your own list of products or movies or books here as csv file#\n",
        "tf = TfidfVectorizer( analyzer = 'word', stop_words=my_stopwords, token_pattern= r'(?u)\\b[A-Za-z]+\\b')\n",
        "\n",
        "\n",
        "#ngram explanation begins#\n",
        "#ngram (1,3) can be explained as follows#\n",
        "#ngram(1,3) encompasses uni gram, bi gram and tri gram\n",
        "#consider the sentence \"The ball fell\"\n",
        "#ngram (1,3) would be the, ball, fell, the ball, ball fell, the ball fell\n",
        "#ngram explanation ends#\n",
        "\n",
        "tfidf_matrix = tf.fit_transform(ds['title'])\n",
        "tfidf_matrix_dense = (tfidf_matrix.todense())\n",
        "\n",
        "\n",
        "my_dict = tf.vocabulary_\n",
        "\n",
        "my_dict['advanced']\n",
        "\n",
        "tf.get_feature_names()\n",
        "\n",
        "cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cosine_similarities\n",
        "results = {} # dictionary created to store the result in a dictionary format (ID : (Score,item_id))#\n",
        "for idx, row in ds.iterrows(): #iterates through all the rows\n",
        "\n",
        "    # print(idx)\n",
        "\n",
        "# the below code 'similar_indice' stores similar ids based on cosine similarity. sorts them in ascending order. [:-5:-1] is then used so that the indices with most similarity are got. 0 means no similarity and 1 means perfect similarity#\n",
        "    similar_indices = cosine_similarities[idx].argsort()[:-5:-1] \n",
        "\n",
        "    #stores 5 most similar books, you can change it as per your needs\n",
        "    similar_items = [(cosine_similarities[idx][i], ds['asin'][i]) for i in similar_indices]\n",
        "    results[row['asin']] = similar_items[1:]\n",
        "\n",
        "# results\n",
        "    \n",
        "#below code 'function item(id)' returns a row matching the id along with Book Title. Initially it is a dataframe, then we convert it to a list#\n",
        "def item(id):\n",
        "    return ds.loc[ds['asin'] == id]['title'].tolist()[0]\n",
        "    \n",
        "def recommend(id, num):\n",
        "    if (num == 0):\n",
        "        print(\"Unable to recommend any book as you have not chosen the number of book to be recommended\")\n",
        "    elif (num==1):\n",
        "        print(\"Recommending \" + str(num) + \" book similar to \" + item(id))\n",
        "        \n",
        "    else :\n",
        "        print(\"Recommending \" + str(num) + \" books similar to \" + item(id))\n",
        "        \n",
        "    print(\"----------------------------------------------------------\")\n",
        "    recs = results[id][:num]\n",
        "    for rec in recs:\n",
        "        print(\"You may also like to read: \" + item(rec[1]) + \" (score:\" + str(rec[0]) + \")\")\n",
        "\n",
        "#the first argument in the below function to be passed is the id of the book, second argument is the number of books you want to be recommended#\n",
        "recommend(5,2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "vDndolvDLznV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TfidfVectorizer(vocabulary=['aqua', 'velva', 'shave', 'classic', 'ice', 'blue',\n",
              "                            'ounc', 'citr', 'shine', 'moistur', 'burst',\n",
              "                            'shampoo', 'fl', 'oz', 'nar', 'blush', 'taj',\n",
              "                            'mahal', 'avalon', 'organ', 'wrinkl', 'therapi',\n",
              "                            'coq', 'cleans', 'milk', 'zum', 'bar', 'anis',\n",
              "                            'lavend', 'yardley', ...])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "(84, 392)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IDF vector length is:  392\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# tfidfvectorizer = TfidfVectorizer(analyzer='word' , stop_words=my_stopwords)\n",
        "#for this i used previously preprocessed vocab.\n",
        "# NOTE: -> NO NUMBER + NO PUNCTUATION + NO STOPWORDS + PORTERSTEMMER + NO DUPLICATES\n",
        "tfidfvectorizer = TfidfVectorizer(vocabulary= vocab)\n",
        "\n",
        "\n",
        "tfidfvectorizer.fit(corpus_train)\n",
        "tfidf_train = tfidfvectorizer.transform(corpus_train)\n",
        "# tfidf_term_vectors  = tfidfvectorizer.transform(test)\n",
        "# print(\"Sparse Matrix form of train data : \\n\")\n",
        "coisa = tfidf_train.todense()\n",
        "coisa.shape\n",
        "\n",
        "print('IDF vector length is: ', len(tfidfvectorizer.idf_))\n",
        "# this means -> weight of words in our my_vocab \n",
        "\n",
        "# multiply TF with IDF row-wise  \n",
        "\n",
        "# WHAT DIMENSIONS MEAN?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(84, 437)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tfidfvectorizer = TfidfVectorizer(analyzer='word' , stop_words=my_stopwords)\n",
        "#for this i used previously preprocessed vocab.\n",
        "# NOTE: -> NO NUMBER + NO PUNCTUATION + NO STOPWORDS + PORTERSTEMMER + NO DUPLICATES\n",
        "\n",
        "# tfidfvectorizer = TfidfVectorizer(analyzer='word', min_df=0, stop_words='english')\n",
        "\n",
        "tfidfvectorizer = TfidfVectorizer(analyzer='word', min_df=0, stop_words='english')\n",
        "tfidf_matrix = tfidfvectorizer.fit_transform(temp['title'])\n",
        "tfidf_matrix.shape\n",
        "\n",
        "# tfidfvectorizer.fit(corpus_train)\n",
        "# tfidf_train = tfidfvectorizer.transform(corpus_train)\n",
        "# # tfidf_term_vectors  = tfidfvectorizer.transform(test)\n",
        "# # print(\"Sparse Matrix form of train data : \\n\")\n",
        "# coisa = tfidf_train.todense()\n",
        "# # coisa\n",
        "\n",
        "# print('IDF vector length is: ', len(tfidfvectorizer.idf_))\n",
        "# # this means -> weight of words in our my_vocab \n",
        "\n",
        "# # multiply TF with IDF row-wise  \n",
        "\n",
        "# # WHAT DIMENSIONS MEAN?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "TD-IDF Vectorizer\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##TRASH!!!\n",
        "\n",
        "# instantiate the vectorizer object\n",
        "# countvectorizer = CountVectorizer(analyzer= 'word', stop_words='english')\n",
        "tfidfvectorizer = TfidfVectorizer(vocabulary=my_vocab, use_idf=True)\n",
        "# convert th documents into a matrix\n",
        "# count_wm = countvectorizer.fit_transform(train)\n",
        "tfidf_wm = tfidfvectorizer.fit_transform(corpus_train)\n",
        "#retrieve the terms found in the corpora\n",
        "# if we take same parameters on both Classes(CountVectorizer and TfidfVectorizer) , it will give same output of get_feature_names() methods)\n",
        "#count_tokens = tfidfvectorizer.get_feature_names() # no difference\n",
        "# count_tokens = countvectorizer.get_feature_names()\n",
        "tfidf_tokens = tfidfvectorizer.get_feature_names()\n",
        "# tfidf_tokens\n",
        "# df_countvect = pd.DataFrame(data = count_wm.toarray(),index = ['Doc1','Doc2'],columns = count_tokens)\n",
        "df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(), columns = tfidf_tokens)\n",
        "# print(\"Count Vectorizer\\n\")\n",
        "# print(df_countvect)\n",
        "print(\"\\nTD-IDF Vectorizer\\n\")\n",
        "# print(df_tfidfvect)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gnTVM0EV_2d"
      },
      "source": [
        "### 2.2\n",
        "\n",
        "Compute and the cosine similarity between products with asin 'B000FI4S1E', 'B000LIBUBY' and 'B000W0C07Y'. Take a look at their features to see whether results make sense with their characteristics. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "zO_OHMY8PWbO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.04176397]])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# cosine_sim\n",
        "\n",
        "new = pd.DataFrame(tfidf_train.todense())\n",
        "\n",
        "new.index = temp['asin']\n",
        "\n",
        "\n",
        "np.array(new.loc['B000LIBUBY']).shape\n",
        "cosine_similarity(np.array([new.loc['B000FI4S1E']]), np.array([new.loc['B000LIBUBY']]))\n",
        "\n",
        "# np.matrix(tfidf_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.04244574]])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cosine_similarity(np.array([new.loc['B000FI4S1E']]), np.array([new.loc['B000W0C07Y']]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.45428816]])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cosine_similarity(np.array([new.loc['B000LIBUBY']]), np.array([new.loc['B000W0C07Y']]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>tech1</th>\n",
              "      <th>description</th>\n",
              "      <th>fit</th>\n",
              "      <th>title</th>\n",
              "      <th>also_buy</th>\n",
              "      <th>tech2</th>\n",
              "      <th>brand</th>\n",
              "      <th>feature</th>\n",
              "      <th>rank</th>\n",
              "      <th>also_view</th>\n",
              "      <th>details</th>\n",
              "      <th>main_cat</th>\n",
              "      <th>similar_item</th>\n",
              "      <th>date</th>\n",
              "      <th>price</th>\n",
              "      <th>asin</th>\n",
              "      <th>imageURL</th>\n",
              "      <th>imageURLHighRes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[INDICATIONS: Aqua Velva Cooling After Shave E...</td>\n",
              "      <td></td>\n",
              "      <td>Aqua Velva After Shave, Classic Ice Blue, 7 Ounce</td>\n",
              "      <td>[B00J232PCM, B0010V5MKG, B000052Y68, B00KOAIU7...</td>\n",
              "      <td></td>\n",
              "      <td>Aqua Velva</td>\n",
              "      <td>[]</td>\n",
              "      <td>65,003 in Beauty &amp; Personal Care (</td>\n",
              "      <td>[B01I9TIY1U, B07L1PZCS7, B01N12C89Y, B01I9TINT...</td>\n",
              "      <td>{'\n",
              "    Product Dimensions: \n",
              "    ': '3 x 4 x 5 ...</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>B0000530HU</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[&lt;P&gt;&lt;STRONG&gt;Restores Moisture to Dehydrated Ha...</td>\n",
              "      <td></td>\n",
              "      <td>Citre Shine Moisture Burst Shampoo - 16 fl oz</td>\n",
              "      <td>[B07CSVCGZV, B07KMGC13Z, B0793XJ4WW, B01N7U1HB...</td>\n",
              "      <td></td>\n",
              "      <td>Citre Shine</td>\n",
              "      <td>[]</td>\n",
              "      <td>1,693,702 in Beauty &amp; Personal Care (</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'ASIN: ': 'B00006L9LC', 'UPC:': '795827187965...</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>$23.00</td>\n",
              "      <td>B00006L9LC</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[A richly pigmented, micronized powder formula...</td>\n",
              "      <td></td>\n",
              "      <td>NARS Blush, Taj Mahal</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>NARS</td>\n",
              "      <td>[]</td>\n",
              "      <td>505,302 in Beauty &amp; Personal Care (</td>\n",
              "      <td>[B07FVJJ39R, B07JBQZDKB, B07HKVJC7G, B010VWL4E...</td>\n",
              "      <td>{'\n",
              "    Item Weight: \n",
              "    ': '0.16 ounces', 'Sh...</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>$34.50</td>\n",
              "      <td>B00021DJ32</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[Avalon Organics Wrinkle Therapy Cleansing Mil...</td>\n",
              "      <td></td>\n",
              "      <td>Avalon Organics Wrinkle Therapy CoQ10 Cleansin...</td>\n",
              "      <td>[B0014407HC, B001ECQ41M, B00503OFIU, B00015XAQ...</td>\n",
              "      <td></td>\n",
              "      <td>Avalon</td>\n",
              "      <td>[]</td>\n",
              "      <td>141,988 in Beauty &amp;amp; Personal Care (</td>\n",
              "      <td>[B077ZG4C3L, B07DW6ZLFS, B00503OFIU, B07DVZMGL...</td>\n",
              "      <td>{'\n",
              "    Product Dimensions: \n",
              "    ': '2.5 x 1.4 ...</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>$8.27</td>\n",
              "      <td>B0002JHI1I</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>881</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[We bring you this all-natural, luxury Zum bar...</td>\n",
              "      <td></td>\n",
              "      <td>ZUM Zum Bar Anise Lavender, 3 Ounce</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>ZUM</td>\n",
              "      <td>[]</td>\n",
              "      <td>586,248 in Beauty &amp; Personal Care (</td>\n",
              "      <td>[B005CYJN0W, B000DN7BW4, B001EXT5J4, B000DLB26...</td>\n",
              "      <td>{'\n",
              "    Product Dimensions: \n",
              "    ': '0.8 x 2.2 ...</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>$7.76</td>\n",
              "      <td>B0006O10P4</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26610</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[new authentic discontinued]</td>\n",
              "      <td></td>\n",
              "      <td>Ultimate Body Lotion By Michael Kors 3.4oz</td>\n",
              "      <td>[B00R1QRWLG]</td>\n",
              "      <td></td>\n",
              "      <td>Michael Kors</td>\n",
              "      <td>[]</td>\n",
              "      <td>736,956 in Beauty &amp; Personal Care (</td>\n",
              "      <td>[B016AGM1MW, B019AWE8TW]</td>\n",
              "      <td>{'ASIN: ': 'B019LAI4HU', 'UPC:': '682821146329'}</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>B019LAI4HU</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26759</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[Launched by the design house of dolce and gab...</td>\n",
              "      <td></td>\n",
              "      <td>Dolce &amp;amp; Gabbana Compact Parfum, 0.05 Ounce</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>Dolce &amp; Gabbana</td>\n",
              "      <td>[]</td>\n",
              "      <td>863,502 in Beauty &amp; Personal Care (</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'\n",
              "    Product Dimensions: \n",
              "    ': '3.5 x 1.2 ...</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>B019V2KYZS</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28095</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[Colgate Kids Maximum Cavity Protection Pump T...</td>\n",
              "      <td></td>\n",
              "      <td>Colgate Kids Maximum Cavity Protection Pump To...</td>\n",
              "      <td>[B07G35BLNY, B01MR4VL2E, B003XDX68E, B002VA4FY...</td>\n",
              "      <td></td>\n",
              "      <td>Colgate</td>\n",
              "      <td>[&lt;span class=\"a-size-base a-color-secondary\"&gt;\\...</td>\n",
              "      <td>307,236 in Beauty &amp; Personal Care (</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'\n",
              "    Product Dimensions: \n",
              "    ': '2 x 0.9 x ...</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>B01BNEYGQU</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29860</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>Bali Secrets Natural Deodorant - Organic &amp;amp;...</td>\n",
              "      <td>[B07DNGSBSC, B00EOB0042, B077F4NB55, B07GXZYBX...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>152,867 in Beauty &amp; Personal Care (</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'\n",
              "    Product Dimensions: \n",
              "    ': '4.3 x 1.8 ...</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>B01DKQAXC0</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30471</th>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[the pleasure is all mine. loose the formaliti...</td>\n",
              "      <td></td>\n",
              "      <td>essie Gel Couture Nail Polish</td>\n",
              "      <td>[B01E7UKPWQ, B01E7UKTAE, B01E7UKS7S, B071RKDH9...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>306,101 in Beauty &amp; Personal Care (</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'\n",
              "    Product Dimensions: \n",
              "    ': '1.5 x 1.5 ...</td>\n",
              "      <td>All Beauty</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>$11.25</td>\n",
              "      <td>B01E7UKR38</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>84 rows × 19 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      category tech1                                        description fit  \\\n",
              "50          []        [INDICATIONS: Aqua Velva Cooling After Shave E...       \n",
              "111         []        [<P><STRONG>Restores Moisture to Dehydrated Ha...       \n",
              "293         []        [A richly pigmented, micronized powder formula...       \n",
              "392         []        [Avalon Organics Wrinkle Therapy Cleansing Mil...       \n",
              "881         []        [We bring you this all-natural, luxury Zum bar...       \n",
              "...        ...   ...                                                ...  ..   \n",
              "26610       []                             [new authentic discontinued]       \n",
              "26759       []        [Launched by the design house of dolce and gab...       \n",
              "28095       []        [Colgate Kids Maximum Cavity Protection Pump T...       \n",
              "29860       []                                                       []       \n",
              "30471       []        [the pleasure is all mine. loose the formaliti...       \n",
              "\n",
              "                                                   title  \\\n",
              "50     Aqua Velva After Shave, Classic Ice Blue, 7 Ounce   \n",
              "111        Citre Shine Moisture Burst Shampoo - 16 fl oz   \n",
              "293                                NARS Blush, Taj Mahal   \n",
              "392    Avalon Organics Wrinkle Therapy CoQ10 Cleansin...   \n",
              "881                  ZUM Zum Bar Anise Lavender, 3 Ounce   \n",
              "...                                                  ...   \n",
              "26610         Ultimate Body Lotion By Michael Kors 3.4oz   \n",
              "26759     Dolce &amp; Gabbana Compact Parfum, 0.05 Ounce   \n",
              "28095  Colgate Kids Maximum Cavity Protection Pump To...   \n",
              "29860  Bali Secrets Natural Deodorant - Organic &amp;...   \n",
              "30471                      essie Gel Couture Nail Polish   \n",
              "\n",
              "                                                also_buy tech2  \\\n",
              "50     [B00J232PCM, B0010V5MKG, B000052Y68, B00KOAIU7...         \n",
              "111    [B07CSVCGZV, B07KMGC13Z, B0793XJ4WW, B01N7U1HB...         \n",
              "293                                                   []         \n",
              "392    [B0014407HC, B001ECQ41M, B00503OFIU, B00015XAQ...         \n",
              "881                                                   []         \n",
              "...                                                  ...   ...   \n",
              "26610                                       [B00R1QRWLG]         \n",
              "26759                                                 []         \n",
              "28095  [B07G35BLNY, B01MR4VL2E, B003XDX68E, B002VA4FY...         \n",
              "29860  [B07DNGSBSC, B00EOB0042, B077F4NB55, B07GXZYBX...         \n",
              "30471  [B01E7UKPWQ, B01E7UKTAE, B01E7UKS7S, B071RKDH9...         \n",
              "\n",
              "                 brand                                            feature  \\\n",
              "50          Aqua Velva                                                 []   \n",
              "111        Citre Shine                                                 []   \n",
              "293               NARS                                                 []   \n",
              "392             Avalon                                                 []   \n",
              "881                ZUM                                                 []   \n",
              "...                ...                                                ...   \n",
              "26610     Michael Kors                                                 []   \n",
              "26759  Dolce & Gabbana                                                 []   \n",
              "28095          Colgate  [<span class=\"a-size-base a-color-secondary\">\\...   \n",
              "29860                                                                  []   \n",
              "30471                                                                  []   \n",
              "\n",
              "                                          rank  \\\n",
              "50          65,003 in Beauty & Personal Care (   \n",
              "111      1,693,702 in Beauty & Personal Care (   \n",
              "293        505,302 in Beauty & Personal Care (   \n",
              "392    141,988 in Beauty &amp; Personal Care (   \n",
              "881        586,248 in Beauty & Personal Care (   \n",
              "...                                        ...   \n",
              "26610      736,956 in Beauty & Personal Care (   \n",
              "26759      863,502 in Beauty & Personal Care (   \n",
              "28095      307,236 in Beauty & Personal Care (   \n",
              "29860      152,867 in Beauty & Personal Care (   \n",
              "30471      306,101 in Beauty & Personal Care (   \n",
              "\n",
              "                                               also_view  \\\n",
              "50     [B01I9TIY1U, B07L1PZCS7, B01N12C89Y, B01I9TINT...   \n",
              "111                                                   []   \n",
              "293    [B07FVJJ39R, B07JBQZDKB, B07HKVJC7G, B010VWL4E...   \n",
              "392    [B077ZG4C3L, B07DW6ZLFS, B00503OFIU, B07DVZMGL...   \n",
              "881    [B005CYJN0W, B000DN7BW4, B001EXT5J4, B000DLB26...   \n",
              "...                                                  ...   \n",
              "26610                           [B016AGM1MW, B019AWE8TW]   \n",
              "26759                                                 []   \n",
              "28095                                                 []   \n",
              "29860                                                 []   \n",
              "30471                                                 []   \n",
              "\n",
              "                                                 details    main_cat  \\\n",
              "50     {'\n",
              "    Product Dimensions: \n",
              "    ': '3 x 4 x 5 ...  All Beauty   \n",
              "111    {'ASIN: ': 'B00006L9LC', 'UPC:': '795827187965...  All Beauty   \n",
              "293    {'\n",
              "    Item Weight: \n",
              "    ': '0.16 ounces', 'Sh...  All Beauty   \n",
              "392    {'\n",
              "    Product Dimensions: \n",
              "    ': '2.5 x 1.4 ...  All Beauty   \n",
              "881    {'\n",
              "    Product Dimensions: \n",
              "    ': '0.8 x 2.2 ...  All Beauty   \n",
              "...                                                  ...         ...   \n",
              "26610   {'ASIN: ': 'B019LAI4HU', 'UPC:': '682821146329'}  All Beauty   \n",
              "26759  {'\n",
              "    Product Dimensions: \n",
              "    ': '3.5 x 1.2 ...  All Beauty   \n",
              "28095  {'\n",
              "    Product Dimensions: \n",
              "    ': '2 x 0.9 x ...  All Beauty   \n",
              "29860  {'\n",
              "    Product Dimensions: \n",
              "    ': '4.3 x 1.8 ...  All Beauty   \n",
              "30471  {'\n",
              "    Product Dimensions: \n",
              "    ': '1.5 x 1.5 ...  All Beauty   \n",
              "\n",
              "      similar_item date   price        asin  \\\n",
              "50                               B0000530HU   \n",
              "111                      $23.00  B00006L9LC   \n",
              "293                      $34.50  B00021DJ32   \n",
              "392                       $8.27  B0002JHI1I   \n",
              "881                       $7.76  B0006O10P4   \n",
              "...            ...  ...     ...         ...   \n",
              "26610                            B019LAI4HU   \n",
              "26759                            B019V2KYZS   \n",
              "28095                            B01BNEYGQU   \n",
              "29860                            B01DKQAXC0   \n",
              "30471                    $11.25  B01E7UKR38   \n",
              "\n",
              "                                                imageURL  \\\n",
              "50     [https://images-na.ssl-images-amazon.com/image...   \n",
              "111                                                   []   \n",
              "293    [https://images-na.ssl-images-amazon.com/image...   \n",
              "392    [https://images-na.ssl-images-amazon.com/image...   \n",
              "881    [https://images-na.ssl-images-amazon.com/image...   \n",
              "...                                                  ...   \n",
              "26610                                                 []   \n",
              "26759  [https://images-na.ssl-images-amazon.com/image...   \n",
              "28095  [https://images-na.ssl-images-amazon.com/image...   \n",
              "29860  [https://images-na.ssl-images-amazon.com/image...   \n",
              "30471  [https://images-na.ssl-images-amazon.com/image...   \n",
              "\n",
              "                                         imageURLHighRes  \n",
              "50     [https://images-na.ssl-images-amazon.com/image...  \n",
              "111                                                   []  \n",
              "293    [https://images-na.ssl-images-amazon.com/image...  \n",
              "392    [https://images-na.ssl-images-amazon.com/image...  \n",
              "881    [https://images-na.ssl-images-amazon.com/image...  \n",
              "...                                                  ...  \n",
              "26610                                                 []  \n",
              "26759  [https://images-na.ssl-images-amazon.com/image...  \n",
              "28095  [https://images-na.ssl-images-amazon.com/image...  \n",
              "29860  [https://images-na.ssl-images-amazon.com/image...  \n",
              "30471  [https://images-na.ssl-images-amazon.com/image...  \n",
              "\n",
              "[84 rows x 19 columns]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temp.loc[temp'asin ']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ATENTION: THIS VALUES ARE ALL HORRIBLY WRONG. \n",
        "MAJOR ISSUES WITH IF-IDF CALCULATION. PROB. CAUSE ON THAT -> MY_VOCAB IN VOCABULARY?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K8jRhWhZQWe"
      },
      "source": [
        "## Exercise 3\n",
        "\n",
        "Representation in vector spaces with contextual Word Embeddings.\n",
        "\n",
        "### 3.1.\n",
        "\n",
        "Represent all the products from Exercise 1 in a vector space using embeddings from a pre-trained BERT model. The final embedding of a product should be the average of the word embeddings from all the words in the 'title'. What is the vocabulary size of the model? What are the dimensions of the last hidden state?\n",
        "\n",
        "Tip: you may install the transformers library and use their pretrained [BERT model uncased](https://huggingface.co/bert-base-uncased)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import transformers\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.modules['transformers'].__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hHIjJ-LbTB3H"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nIf you plan on using a pretrained model, it’s important to use the associated \\npretrained tokenizer: it will split the text you give it in tokens the same way\\nfor the pretraining corpus, and it will use the same correspondence\\ntoken to index (that we usually call a vocab) as during pretraining.\\n'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# LOAD TRANSFORMER\n",
        "\"\"\"\n",
        "If you plan on using a pretrained model, it’s important to use the associated \n",
        "pretrained tokenizer: it will split the text you give it in tokens the same way\n",
        "for the pretraining corpus, and it will use the same correspondence\n",
        "token to index (that we usually call a vocab) as during pretraining.\n",
        "\"\"\"\n",
        "\n",
        "# % pip install transformers\n",
        "import torch\n",
        "import transformers\n",
        "assert transformers.__version__ > '4.0.0'\n",
        "\n",
        "from transformers import BertModel, BertTokenizerFast, BertTokenizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size of:  30522\n"
          ]
        }
      ],
      "source": [
        "# set-up environment\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "# print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "\n",
        "modelname = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(modelname)\n",
        "# model = BertModel.from_pretrained(modelname).to(DEVICE)\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased',\n",
        "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        "                                  )\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "# model.eval()\n",
        "\n",
        "\n",
        "# Print out the vocabulary size of model\n",
        "print('Vocabulary size of: ', len(tokenizer.vocab.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'size'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\franc\\PycharmProjects\\WebScicence_FinalProject\\labs\\session_1\\Session_1.ipynb Cell 121'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franc/PycharmProjects/WebScicence_FinalProject/labs/session_1/Session_1.ipynb#ch0000131?line=0'>1</a>\u001b[0m coisa \u001b[39m=\u001b[39m tokenizer(corpus_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franc/PycharmProjects/WebScicence_FinalProject/labs/session_1/Session_1.ipynb#ch0000131?line=1'>2</a>\u001b[0m \u001b[39m# coisa\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/franc/PycharmProjects/WebScicence_FinalProject/labs/session_1/Session_1.ipynb#ch0000131?line=2'>3</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcoisa)\n",
            "File \u001b[1;32m~\\miniconda3\\envs\\snowflakes\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/franc/miniconda3/envs/snowflakes/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/franc/miniconda3/envs/snowflakes/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/franc/miniconda3/envs/snowflakes/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/franc/miniconda3/envs/snowflakes/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/franc/miniconda3/envs/snowflakes/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/franc/miniconda3/envs/snowflakes/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/franc/miniconda3/envs/snowflakes/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32m~\\miniconda3\\envs\\snowflakes\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:943\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/franc/miniconda3/envs/snowflakes/lib/site-packages/transformers/models/bert/modeling_bert.py?line=940'>941</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/franc/miniconda3/envs/snowflakes/lib/site-packages/transformers/models/bert/modeling_bert.py?line=941'>942</a>\u001b[0m \u001b[39melif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/franc/miniconda3/envs/snowflakes/lib/site-packages/transformers/models/bert/modeling_bert.py?line=942'>943</a>\u001b[0m     input_shape \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39;49msize()\n\u001b[0;32m    <a href='file:///c%3A/Users/franc/miniconda3/envs/snowflakes/lib/site-packages/transformers/models/bert/modeling_bert.py?line=943'>944</a>\u001b[0m \u001b[39melif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/franc/miniconda3/envs/snowflakes/lib/site-packages/transformers/models/bert/modeling_bert.py?line=944'>945</a>\u001b[0m     input_shape \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
          ]
        }
      ],
      "source": [
        "coisa = tokenizer(corpus_train)\n",
        "# coisa\n",
        "outputs = model(**coisa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "BertModel object argument after ** must be a mapping, not Tensor",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\franc\\PycharmProjects\\WebScicence_FinalProject\\labs\\session_1\\Session_1.ipynb Cell 121'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franc/PycharmProjects/WebScicence_FinalProject/labs/session_1/Session_1.ipynb#ch0000129?line=0'>1</a>\u001b[0m encoding \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(corpus_train, add_special_tokens \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, truncation \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, padding \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, return_attention_mask \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, return_tensors \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/franc/PycharmProjects/WebScicence_FinalProject/labs/session_1/Session_1.ipynb#ch0000129?line=1'>2</a>\u001b[0m output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoding)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franc/PycharmProjects/WebScicence_FinalProject/labs/session_1/Session_1.ipynb#ch0000129?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franc/PycharmProjects/WebScicence_FinalProject/labs/session_1/Session_1.ipynb#ch0000129?line=5'>6</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franc/PycharmProjects/WebScicence_FinalProject/labs/session_1/Session_1.ipynb#ch0000129?line=6'>7</a>\u001b[0m     \u001b[39m# outputs = model(tokens_tensor, segments_tensors)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franc/PycharmProjects/WebScicence_FinalProject/labs/session_1/Session_1.ipynb#ch0000129?line=7'>8</a>\u001b[0m     output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoding)\n",
            "\u001b[1;31mTypeError\u001b[0m: BertModel object argument after ** must be a mapping, not Tensor"
          ]
        }
      ],
      "source": [
        "encoding = tokenizer.encode_plus(corpus_train, add_special_tokens = True, truncation = True, padding = \"max_length\", return_attention_mask = True, return_tensors = \"pt\")\n",
        "output = model(**encoding)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    # outputs = model(tokens_tensor, segments_tensors)\n",
        "    output = model(**encoding)\n",
        "\n",
        "\n",
        "    # Evaluating the model will return a different number of objects based on \n",
        "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
        "    # becase we set `output_hidden_states = True`, the third item will be the \n",
        "    # hidden states from all layers. See the documentation for more details:\n",
        "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "    hidden_states = output[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(hidden_states)\n",
        "len(hidden_states[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# text = \"Here is the sentence I want embeddings for.\"\n",
        "# marked_text = \"[CLS] \" + corpus_train + \" [SEP]\"\n",
        "\n",
        "sequence = ' [SEP] '.join(corpus_train)\n",
        "marked_text = '[CLS] ' + sequence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# # Print out the tokens.\n",
        "# (tokenized_text)\n",
        "# (tokenizer.vocab.keys())\n",
        "\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# Display the words with their indeces.\n",
        "for tup in zip(tokenized_text, indexed_tokens):\n",
        "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n",
        "\n",
        "\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "(tokens_tensor).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# encoded = tokenizer.encode(tokenized_text)\n",
        "# print(encoded.ids)\n",
        "\n",
        "inputs = tokenizer(marked_text)\n",
        "\n",
        "\n",
        "inputs\n",
        "encoded_sequence = inputs[\"input_ids\"]\n",
        "\n",
        "\n",
        "tokenizer.decode(encoded_sequence)\n",
        "# corpus_train\n",
        "\n",
        "# coisa = ' [SEP] '.join(corpus_train)\n",
        "# masked_text = '[CLS] ' + coisa\n",
        "\n",
        "# masked_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feed text directly to tokenize. We then get:\n",
        "# -> \"input_ids\",\n",
        "# -> \"attention_mask\" (84 x 84 matrix of 0's and 1's. each row represents a title entrance in the 'corpus_train' )\n",
        "# -> \"token_type_ids\"\n",
        "\n",
        "\n",
        "\n",
        "#do this when there's more then one sentence. Add \"padding\" to add \"SEP\" between sentences. BERT expects that\n",
        "\n",
        "padded_sequences = tokenizer(corpus_train, padding=True)\n",
        "\n",
        "(padded_sequences['attention_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sequence_a = \"HuggingFace is based in NYC\"\n",
        "sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\n",
        "sequence_c = \"HuggingFace is based in LA!\"\n",
        "\n",
        "\n",
        "encoded_dict = tokenizer(sequence_a, sequence_b, sequence_c)\n",
        "# encoded_dict[\"input_ids\"]\n",
        "encoded_dict \n",
        "\n",
        "tokenizer.decode(encoded_dict[\"input_ids\"])\n",
        "# print(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_Symyv5U07x"
      },
      "outputs": [],
      "source": [
        "# REPRESENT PRODUCTS IN A VECTOR SPACE\n",
        "\n",
        "\n",
        "def batch_encoding(sentences):\n",
        "    # Since we're using padding, we need to provide the attention masks to our\n",
        "    # model. Otherwise it doesn't know which tokens it should not attend to. \n",
        "    inputs = # <YOUR CODE HERE>\n",
        "    # print(inputs) # Look at the padding and attention_mask\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    last_hidden_states = # <YOUR CODE HERE>\n",
        "\n",
        "    return inputs, last_hidden_states\n",
        "  \n",
        "encoded_inputs, title_last_hidden_states = batch_encoding( # <YOUR CODE HERE> )\n",
        "\n",
        "\"\"\"\n",
        "Note that the control token [CLS] has been added \n",
        "at the beginning of each sentence, and [SEP] at the end. \n",
        "\"\"\"\n",
        "\n",
        "# Now, let's mask out the padding tokens and compute the embedding vector of each product\n",
        "\n",
        "# <YOUR CODE HERE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwRBr2HP0Zdt"
      },
      "source": [
        "### 3.2.\n",
        "\n",
        "Compute and the cosine similarity between products with asin 'B000FI4S1E', 'B000LIBUBY' and 'B000W0C07Y'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaHxSLHqItNs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ae6011ca",
      "metadata": {},
      "source": [
        "# Content-based recommendation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a455b8a",
      "metadata": {},
      "source": [
        "## Exercise 1\n",
        "Based on the TF-IDF vectors obtained in the Exercise 2 from Session 4, represent each user in the same vector space. Amongst other feasible solutions, you can represent a user (user profile) by computing the weighted mean of the items vectors. Compute the cosine similarity for user 'A39WWMBA0299ZF' and all products in the training set not rated by the user. What are the top-5 recommended items for user 'A39WWMBA0299ZF'? Print out the top-5 items and their similarity score.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "id": "403ff409",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of users loaded:  981\n"
          ]
        }
      ],
      "source": [
        "# access items in df 5_core\n",
        "bebe = Data(df)\n",
        "\n",
        "users_train = bebe.get_train()['reviewerID'].unique()\n",
        "users_test = bebe.get_test()['reviewerID'].unique()\n",
        "\n",
        "users = pd.DataFrame(np.concatenate((users_train, users_test)))[0].unique()\n",
        "\n",
        "#how many users have the items\n",
        "print('number of users loaded: ', len(users) )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50       B0000530HU\n",
              "111      B00006L9LC\n",
              "293      B00021DJ32\n",
              "392      B0002JHI1I\n",
              "881      B0006O10P4\n",
              "            ...    \n",
              "26610    B019LAI4HU\n",
              "26759    B019V2KYZS\n",
              "28095    B01BNEYGQU\n",
              "29860    B01DKQAXC0\n",
              "30471    B01E7UKR38\n",
              "Name: asin, Length: 84, dtype: object"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temp['asin']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>asin</th>\n",
              "      <th>B0000530HU</th>\n",
              "      <th>B00006L9LC</th>\n",
              "      <th>B00021DJ32</th>\n",
              "      <th>B0002JHI1I</th>\n",
              "      <th>B0006O10P4</th>\n",
              "      <th>B0009RF9DW</th>\n",
              "      <th>B000FI4S1E</th>\n",
              "      <th>B000FOI48G</th>\n",
              "      <th>B000FTYALG</th>\n",
              "      <th>B000GLRREU</th>\n",
              "      <th>...</th>\n",
              "      <th>B00VG1AV5Q</th>\n",
              "      <th>B00W259T7G</th>\n",
              "      <th>B016V8YWBC</th>\n",
              "      <th>B019809F9Y</th>\n",
              "      <th>B019FWRG3C</th>\n",
              "      <th>B019LAI4HU</th>\n",
              "      <th>B019V2KYZS</th>\n",
              "      <th>B01BNEYGQU</th>\n",
              "      <th>B01DKQAXC0</th>\n",
              "      <th>B01E7UKR38</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>reviewerID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>A105A034ZG9EHO</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A10JB7YPWZGRF4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A10M2MLE2R0L6K</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A10P0NAKKRYKTZ</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A10ZJZNO4DAVB</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AZCOSCQG73JZ1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AZD3ON9ZMEGL6</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AZFYUPGEE6KLW</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AZJMUP77WBQZQ</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AZRD4IZU6TBFV</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>981 rows × 84 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "asin            B0000530HU  B00006L9LC  B00021DJ32  B0002JHI1I  B0006O10P4  \\\n",
              "reviewerID                                                                   \n",
              "A105A034ZG9EHO         0.0         0.0         0.0         0.0         0.0   \n",
              "A10JB7YPWZGRF4         0.0         0.0         0.0         0.0         0.0   \n",
              "A10M2MLE2R0L6K         0.0         0.0         0.0         0.0         0.0   \n",
              "A10P0NAKKRYKTZ         0.0         0.0         0.0         0.0         0.0   \n",
              "A10ZJZNO4DAVB          0.0         5.0         0.0         0.0         0.0   \n",
              "...                    ...         ...         ...         ...         ...   \n",
              "AZCOSCQG73JZ1          0.0         5.0         0.0         0.0         0.0   \n",
              "AZD3ON9ZMEGL6          0.0         0.0         0.0         0.0         0.0   \n",
              "AZFYUPGEE6KLW          0.0         5.0         0.0         0.0         0.0   \n",
              "AZJMUP77WBQZQ          0.0         5.0         0.0         0.0         0.0   \n",
              "AZRD4IZU6TBFV          0.0         0.0         0.0         0.0         0.0   \n",
              "\n",
              "asin            B0009RF9DW  B000FI4S1E  B000FOI48G  B000FTYALG  B000GLRREU  \\\n",
              "reviewerID                                                                   \n",
              "A105A034ZG9EHO         5.0         5.0         0.0         0.0         0.0   \n",
              "A10JB7YPWZGRF4         5.0         5.0         0.0         0.0         0.0   \n",
              "A10M2MLE2R0L6K         0.0         0.0         0.0         0.0         0.0   \n",
              "A10P0NAKKRYKTZ         5.0         5.0         0.0         0.0         0.0   \n",
              "A10ZJZNO4DAVB          0.0         0.0         0.0         0.0         0.0   \n",
              "...                    ...         ...         ...         ...         ...   \n",
              "AZCOSCQG73JZ1          0.0         0.0         0.0         0.0         0.0   \n",
              "AZD3ON9ZMEGL6          5.0         5.0         0.0         0.0         0.0   \n",
              "AZFYUPGEE6KLW          0.0         0.0         0.0         0.0         0.0   \n",
              "AZJMUP77WBQZQ          0.0         0.0         0.0         0.0         0.0   \n",
              "AZRD4IZU6TBFV          5.0         5.0         0.0         0.0         0.0   \n",
              "\n",
              "asin            ...  B00VG1AV5Q  B00W259T7G  B016V8YWBC  B019809F9Y  \\\n",
              "reviewerID      ...                                                   \n",
              "A105A034ZG9EHO  ...         0.0         0.0         0.0         0.0   \n",
              "A10JB7YPWZGRF4  ...         0.0         0.0         0.0         0.0   \n",
              "A10M2MLE2R0L6K  ...         0.0         5.0         0.0         0.0   \n",
              "A10P0NAKKRYKTZ  ...         0.0         0.0         0.0         0.0   \n",
              "A10ZJZNO4DAVB   ...         0.0         0.0         0.0         0.0   \n",
              "...             ...         ...         ...         ...         ...   \n",
              "AZCOSCQG73JZ1   ...         0.0         0.0         0.0         0.0   \n",
              "AZD3ON9ZMEGL6   ...         0.0         0.0         0.0         0.0   \n",
              "AZFYUPGEE6KLW   ...         0.0         0.0         0.0         0.0   \n",
              "AZJMUP77WBQZQ   ...         0.0         0.0         0.0         0.0   \n",
              "AZRD4IZU6TBFV   ...         0.0         0.0         0.0         0.0   \n",
              "\n",
              "asin            B019FWRG3C  B019LAI4HU  B019V2KYZS  B01BNEYGQU  B01DKQAXC0  \\\n",
              "reviewerID                                                                   \n",
              "A105A034ZG9EHO         0.0         0.0         0.0         0.0         0.0   \n",
              "A10JB7YPWZGRF4         0.0         0.0         0.0         0.0         0.0   \n",
              "A10M2MLE2R0L6K         5.0         0.0         0.0         0.0         0.0   \n",
              "A10P0NAKKRYKTZ         0.0         0.0         0.0         0.0         0.0   \n",
              "A10ZJZNO4DAVB          0.0         0.0         0.0         0.0         0.0   \n",
              "...                    ...         ...         ...         ...         ...   \n",
              "AZCOSCQG73JZ1          0.0         0.0         0.0         0.0         0.0   \n",
              "AZD3ON9ZMEGL6          0.0         0.0         0.0         0.0         0.0   \n",
              "AZFYUPGEE6KLW          0.0         0.0         0.0         0.0         0.0   \n",
              "AZJMUP77WBQZQ          0.0         0.0         0.0         0.0         0.0   \n",
              "AZRD4IZU6TBFV          0.0         0.0         0.0         0.0         0.0   \n",
              "\n",
              "asin            B01E7UKR38  \n",
              "reviewerID                  \n",
              "A105A034ZG9EHO         0.0  \n",
              "A10JB7YPWZGRF4         0.0  \n",
              "A10M2MLE2R0L6K         0.0  \n",
              "A10P0NAKKRYKTZ         0.0  \n",
              "A10ZJZNO4DAVB          0.0  \n",
              "...                    ...  \n",
              "AZCOSCQG73JZ1          0.0  \n",
              "AZD3ON9ZMEGL6          0.0  \n",
              "AZFYUPGEE6KLW          0.0  \n",
              "AZJMUP77WBQZQ          0.0  \n",
              "AZRD4IZU6TBFV          0.0  \n",
              "\n",
              "[981 rows x 84 columns]"
            ]
          },
          "execution_count": 207,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>action</th>\n",
              "      <th>advanced</th>\n",
              "      <th>aerosol</th>\n",
              "      <th>ageless</th>\n",
              "      <th>aging</th>\n",
              "      <th>air</th>\n",
              "      <th>allergenic</th>\n",
              "      <th>almond</th>\n",
              "      <th>american</th>\n",
              "      <th>amp</th>\n",
              "      <th>...</th>\n",
              "      <th>wintergreen</th>\n",
              "      <th>wiseways</th>\n",
              "      <th>witch</th>\n",
              "      <th>women</th>\n",
              "      <th>woods</th>\n",
              "      <th>works</th>\n",
              "      <th>wrinkle</th>\n",
              "      <th>yardley</th>\n",
              "      <th>youthful</th>\n",
              "      <th>zum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>A105A034ZG9EHO</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.904911</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.348573</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.998251</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A10JB7YPWZGRF4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.904911</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.348573</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.998251</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A10M2MLE2R0L6K</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.957750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A10P0NAKKRYKTZ</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.904911</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.348573</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.998251</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A10ZJZNO4DAVB</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.796052</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.348573</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AZCOSCQG73JZ1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.796052</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.348573</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AZD3ON9ZMEGL6</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.904911</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.348573</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.998251</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AZFYUPGEE6KLW</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.796052</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.348573</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AZJMUP77WBQZQ</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.796052</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.348573</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AZRD4IZU6TBFV</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.904911</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.348573</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.998251</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>981 rows × 402 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                action  advanced  aerosol  ageless  aging  air  allergenic  \\\n",
              "A105A034ZG9EHO     0.0       0.0      0.0      0.0    0.0  0.0         0.0   \n",
              "A10JB7YPWZGRF4     0.0       0.0      0.0      0.0    0.0  0.0         0.0   \n",
              "A10M2MLE2R0L6K     0.0       0.0      0.0      0.0    0.0  0.0         0.0   \n",
              "A10P0NAKKRYKTZ     0.0       0.0      0.0      0.0    0.0  0.0         0.0   \n",
              "A10ZJZNO4DAVB      0.0       0.0      0.0      0.0    0.0  0.0         0.0   \n",
              "...                ...       ...      ...      ...    ...  ...         ...   \n",
              "AZCOSCQG73JZ1      0.0       0.0      0.0      0.0    0.0  0.0         0.0   \n",
              "AZD3ON9ZMEGL6      0.0       0.0      0.0      0.0    0.0  0.0         0.0   \n",
              "AZFYUPGEE6KLW      0.0       0.0      0.0      0.0    0.0  0.0         0.0   \n",
              "AZJMUP77WBQZQ      0.0       0.0      0.0      0.0    0.0  0.0         0.0   \n",
              "AZRD4IZU6TBFV      0.0       0.0      0.0      0.0    0.0  0.0         0.0   \n",
              "\n",
              "                almond  american       amp  ...  wintergreen  wiseways  witch  \\\n",
              "A105A034ZG9EHO     0.0       0.0  2.904911  ...          0.0       0.0    0.0   \n",
              "A10JB7YPWZGRF4     0.0       0.0  2.904911  ...          0.0       0.0    0.0   \n",
              "A10M2MLE2R0L6K     0.0       0.0  1.957750  ...          0.0       0.0    0.0   \n",
              "A10P0NAKKRYKTZ     0.0       0.0  2.904911  ...          0.0       0.0    0.0   \n",
              "A10ZJZNO4DAVB      0.0       0.0  0.796052  ...          0.0       0.0    0.0   \n",
              "...                ...       ...       ...  ...          ...       ...    ...   \n",
              "AZCOSCQG73JZ1      0.0       0.0  0.796052  ...          0.0       0.0    0.0   \n",
              "AZD3ON9ZMEGL6      0.0       0.0  2.904911  ...          0.0       0.0    0.0   \n",
              "AZFYUPGEE6KLW      0.0       0.0  0.796052  ...          0.0       0.0    0.0   \n",
              "AZJMUP77WBQZQ      0.0       0.0  0.796052  ...          0.0       0.0    0.0   \n",
              "AZRD4IZU6TBFV      0.0       0.0  2.904911  ...          0.0       0.0    0.0   \n",
              "\n",
              "                women  woods     works  wrinkle   yardley  youthful  zum  \n",
              "A105A034ZG9EHO    0.0    0.0  1.348573      0.0  2.998251       0.0  0.0  \n",
              "A10JB7YPWZGRF4    0.0    0.0  1.348573      0.0  2.998251       0.0  0.0  \n",
              "A10M2MLE2R0L6K    0.0    0.0  0.000000      0.0  0.000000       0.0  0.0  \n",
              "A10P0NAKKRYKTZ    0.0    0.0  1.348573      0.0  2.998251       0.0  0.0  \n",
              "A10ZJZNO4DAVB     0.0    0.0  1.348573      0.0  0.000000       0.0  0.0  \n",
              "...               ...    ...       ...      ...       ...       ...  ...  \n",
              "AZCOSCQG73JZ1     0.0    0.0  1.348573      0.0  0.000000       0.0  0.0  \n",
              "AZD3ON9ZMEGL6     0.0    0.0  1.348573      0.0  2.998251       0.0  0.0  \n",
              "AZFYUPGEE6KLW     0.0    0.0  1.348573      0.0  0.000000       0.0  0.0  \n",
              "AZJMUP77WBQZQ     0.0    0.0  1.348573      0.0  0.000000       0.0  0.0  \n",
              "AZRD4IZU6TBFV     0.0    0.0  1.348573      0.0  2.998251       0.0  0.0  \n",
              "\n",
              "[981 rows x 402 columns]"
            ]
          },
          "execution_count": 207,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>action</th>\n",
              "      <th>advanced</th>\n",
              "      <th>aerosol</th>\n",
              "      <th>ageless</th>\n",
              "      <th>aging</th>\n",
              "      <th>air</th>\n",
              "      <th>allergenic</th>\n",
              "      <th>almond</th>\n",
              "      <th>american</th>\n",
              "      <th>amp</th>\n",
              "      <th>...</th>\n",
              "      <th>wintergreen</th>\n",
              "      <th>wiseways</th>\n",
              "      <th>witch</th>\n",
              "      <th>women</th>\n",
              "      <th>woods</th>\n",
              "      <th>works</th>\n",
              "      <th>wrinkle</th>\n",
              "      <th>yardley</th>\n",
              "      <th>youthful</th>\n",
              "      <th>zum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>A105A034ZG9EHO</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.904911</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.348573</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.998251</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A10JB7YPWZGRF4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.904911</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.348573</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.998251</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A10M2MLE2R0L6K</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.957750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A10P0NAKKRYKTZ</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.904911</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.348573</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.998251</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A10ZJZNO4DAVB</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.796052</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.348573</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AZCOSCQG73JZ1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.796052</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.348573</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AZD3ON9ZMEGL6</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.904911</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.348573</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.998251</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AZFYUPGEE6KLW</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.796052</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.348573</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AZJMUP77WBQZQ</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.796052</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.348573</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AZRD4IZU6TBFV</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.904911</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.348573</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.998251</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>981 rows × 402 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                action  advanced  aerosol  ageless  aging  air  allergenic  \\\n",
              "A105A034ZG9EHO     0.0       0.0      0.0      0.0    0.0  0.0         0.0   \n",
              "A10JB7YPWZGRF4     0.0       0.0      0.0      0.0    0.0  0.0         0.0   \n",
              "A10M2MLE2R0L6K     0.0       0.0      0.0      0.0    0.0  0.0         0.0   \n",
              "A10P0NAKKRYKTZ     0.0       0.0      0.0      0.0    0.0  0.0         0.0   \n",
              "A10ZJZNO4DAVB      0.0       0.0      0.0      0.0    0.0  0.0         0.0   \n",
              "...                ...       ...      ...      ...    ...  ...         ...   \n",
              "AZCOSCQG73JZ1      0.0       0.0      0.0      0.0    0.0  0.0         0.0   \n",
              "AZD3ON9ZMEGL6      0.0       0.0      0.0      0.0    0.0  0.0         0.0   \n",
              "AZFYUPGEE6KLW      0.0       0.0      0.0      0.0    0.0  0.0         0.0   \n",
              "AZJMUP77WBQZQ      0.0       0.0      0.0      0.0    0.0  0.0         0.0   \n",
              "AZRD4IZU6TBFV      0.0       0.0      0.0      0.0    0.0  0.0         0.0   \n",
              "\n",
              "                almond  american       amp  ...  wintergreen  wiseways  witch  \\\n",
              "A105A034ZG9EHO     0.0       0.0  2.904911  ...          0.0       0.0    0.0   \n",
              "A10JB7YPWZGRF4     0.0       0.0  2.904911  ...          0.0       0.0    0.0   \n",
              "A10M2MLE2R0L6K     0.0       0.0  1.957750  ...          0.0       0.0    0.0   \n",
              "A10P0NAKKRYKTZ     0.0       0.0  2.904911  ...          0.0       0.0    0.0   \n",
              "A10ZJZNO4DAVB      0.0       0.0  0.796052  ...          0.0       0.0    0.0   \n",
              "...                ...       ...       ...  ...          ...       ...    ...   \n",
              "AZCOSCQG73JZ1      0.0       0.0  0.796052  ...          0.0       0.0    0.0   \n",
              "AZD3ON9ZMEGL6      0.0       0.0  2.904911  ...          0.0       0.0    0.0   \n",
              "AZFYUPGEE6KLW      0.0       0.0  0.796052  ...          0.0       0.0    0.0   \n",
              "AZJMUP77WBQZQ      0.0       0.0  0.796052  ...          0.0       0.0    0.0   \n",
              "AZRD4IZU6TBFV      0.0       0.0  2.904911  ...          0.0       0.0    0.0   \n",
              "\n",
              "                women  woods     works  wrinkle   yardley  youthful  zum  \n",
              "A105A034ZG9EHO    0.0    0.0  1.348573      0.0  2.998251       0.0  0.0  \n",
              "A10JB7YPWZGRF4    0.0    0.0  1.348573      0.0  2.998251       0.0  0.0  \n",
              "A10M2MLE2R0L6K    0.0    0.0  0.000000      0.0  0.000000       0.0  0.0  \n",
              "A10P0NAKKRYKTZ    0.0    0.0  1.348573      0.0  2.998251       0.0  0.0  \n",
              "A10ZJZNO4DAVB     0.0    0.0  1.348573      0.0  0.000000       0.0  0.0  \n",
              "...               ...    ...       ...      ...       ...       ...  ...  \n",
              "AZCOSCQG73JZ1     0.0    0.0  1.348573      0.0  0.000000       0.0  0.0  \n",
              "AZD3ON9ZMEGL6     0.0    0.0  1.348573      0.0  2.998251       0.0  0.0  \n",
              "AZFYUPGEE6KLW     0.0    0.0  1.348573      0.0  0.000000       0.0  0.0  \n",
              "AZJMUP77WBQZQ     0.0    0.0  1.348573      0.0  0.000000       0.0  0.0  \n",
              "AZRD4IZU6TBFV     0.0    0.0  1.348573      0.0  2.998251       0.0  0.0  \n",
              "\n",
              "[981 rows x 402 columns]"
            ]
          },
          "execution_count": 207,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "almost = user_item_matrix(train_and_test)\n",
        "\n",
        "#make sure columns (items) order is correct, before matrix multiplication\n",
        "almost = almost[list(temp['asin'])]\n",
        "\n",
        "almost \n",
        "\n",
        "#get number of rated items per user to use later in the weighted vectorization\n",
        "oi= (almost != 0).astype(int).sum(axis=1) \n",
        "# make df to later append\n",
        "oi = pd.DataFrame((oi), index = oi.index)\n",
        "\n",
        "\n",
        "almost_matrix = almost.to_numpy()\n",
        "# Multiply(981,84) * (84, 402) -> (981, 402)\n",
        "user_vectors_matrix = np.dot(almost_matrix,tfidf_matrix_dense)\n",
        "\n",
        "\n",
        "user_vectors_df = pd.DataFrame(user_vectors_matrix, columns = tf.get_feature_names(), index = list(almost.index))\n",
        "\n",
        "user_vectors_df\n",
        "\n",
        "user_vectors_df['count'] = oi\n",
        "\n",
        "\n",
        "final = user_vectors_df.div(user_vectors_df['count'], axis = 0).drop(columns = ['count'])\n",
        "\n",
        "\n",
        "user_vectors_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_user_vectors(tfidf_matrix_dense, meta_data):\n",
        "\n",
        "    \"\"\" Multiply (users, items) * (items, features) -> (users, features)\n",
        "    Then, divide by number of ratings per user to get Weighted mean of user vectors\"\"\"\n",
        "\n",
        "    # get user_item_matrix. From Collaborative Filtering\n",
        "    almost = user_item_matrix(train_and_test)\n",
        "\n",
        "    #make sure columns (items) order is correct, before matrix multiplication\n",
        "    almost = almost[list(meta_data['asin'])]\n",
        "\n",
        "    almost_matrix = almost.to_numpy()\n",
        "\n",
        "    #get number of rated items per user to use later in the weighted vectorization\n",
        "    count = (almost != 0).astype(int).sum(axis=1) \n",
        "    # make df to later append\n",
        "    count = pd.DataFrame((count), index = count.index)\n",
        "\n",
        "\n",
        "    # Multiply(981,84) * (84, 402) -> (981, 402)\n",
        "    user_vectors_matrix = np.dot(almost_matrix,tfidf_matrix_dense)\n",
        "\n",
        "    user_vectors_df = pd.DataFrame(user_vectors_matrix, columns = tf.get_feature_names(), index = list(almost.index))\n",
        "\n",
        "    # Append 'count' df \n",
        "    user_vectors_df['count98'] = count\n",
        "\n",
        "    # for given user divide all features by count\n",
        "    final = user_vectors_df.div(user_vectors_df['count98'], axis = 0).drop(columns = ['count98'])\n",
        "\n",
        "\n",
        "    return final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(84, 402)"
            ]
          },
          "execution_count": 210,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "(84, 84)"
            ]
          },
          "execution_count": 210,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf_matrix_dense.shape\n",
        "\n",
        "cosine_similarities.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "599f2bbe",
      "metadata": {},
      "source": [
        "## Exercise 2\n",
        "Compute the systems’ hit rate based on the top-5, top-10 and top-20 recommendations, averaged over the total number of users. Remember that, as we are evaluating the system, you should compute the hit rate over the test set. How well/bad does this Content-based approach perform compared to the Collaborative Filtering?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7455795",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b1d74d43",
      "metadata": {},
      "source": [
        "## Exercise 3\n",
        "\n",
        "Repeat Exercise 1 and 2, this time representing the products and users in a word2vec vector space. You may use the gensim library and download the 300-dimension embeddings from Google. Source: https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca33a843",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gensim.downloader\n",
        "word2vec_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "Session_1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "0f9cfdd72e9992f909e95dd59f40a3418d08e1877421f3ae0567c38eb6c8a49a"
    },
    "kernelspec": {
      "display_name": "R",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
